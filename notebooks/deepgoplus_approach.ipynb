{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8234965-a6fc-4a6c-81b5-461fafe55d01",
   "metadata": {},
   "source": [
    "ƒê√¢y l√† ki·∫øn tr√∫c CNN ch√∫ng ta s·∫Ω x√¢y d·ª±ng (d·ª±a tr√™n m√¥ t·∫£ c·ªßa b√†i b√°o ):\n",
    "\n",
    "Input: Sequence d·∫°ng One-Hot Encoding (Ma tr·∫≠n 21 x ƒê·ªô d√†i chu·ªói).\n",
    "\n",
    "Conv Layers: Nhi·ªÅu b·ªô l·ªçc v·ªõi k√≠ch th∆∞·ªõc kh√°c nhau (v√≠ d·ª•: qu√©t 8, 16, 24, 32 axit amin c√πng l√∫c) ƒë·ªÉ b·∫Øt c√°c motif d√†i ng·∫Øn kh√°c nhau.\n",
    "\n",
    "Pooling: Max Pooling ƒë·ªÉ ch·ªâ gi·ªØ l·∫°i t√≠n hi·ªáu m·∫°nh nh·∫•t c·ªßa motif.\n",
    "\n",
    "Output: D·ª± ƒëo√°n nh√£n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1652166-f9ff-4d29-92e8-e5bef463bf50",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32c4ad0-107d-4a31-af4b-34dde876af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Encoding /workspace/data/Train/train_sequences.fasta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82404it [00:01, 49812.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Processing Labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76297it [00:00, 304256.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ready: X=(82404, 1000), Y=(82404, 1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "TRAIN_FASTA = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "TRAIN_TERMS = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_FASTA  = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "MODEL_PATH  = \"deepgoplus_model.pth\"\n",
    "\n",
    "MAX_LEN = 1000   # DeepGOPlus th∆∞·ªùng d√πng ƒë·ªô d√†i c·ªë ƒë·ªãnh (v√≠ d·ª• 2000, ta d√πng 1000 cho nh·∫π)\n",
    "BATCH_SIZE = 64  # TƒÉng batch size v√¨ CNN nh·∫π h∆°n Transformer nhi·ªÅu\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\" # 20 lo·∫°i axit amin chu·∫©n\n",
    "\n",
    "# Map: K√Ω t·ª± -> S·ªë nguy√™n (1-20), 0 ƒë·ªÉ padding\n",
    "aa_to_id = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "def get_one_hot_data(fasta_file, ids_filter=None):\n",
    "    sequences = []\n",
    "    ids = []\n",
    "    print(f\"‚è≥ Encoding {fasta_file}...\")\n",
    "    \n",
    "    for record in tqdm(SeqIO.parse(fasta_file, \"fasta\")):\n",
    "        pid = str(record.id).split(\"|\")[1] if \"|\" in str(record.id) else str(record.id)\n",
    "        \n",
    "        if ids_filter is not None and pid not in ids_filter:\n",
    "            continue\n",
    "            \n",
    "        seq = str(record.seq)\n",
    "        # Encode sang s·ªë\n",
    "        encoded_seq = [aa_to_id.get(aa, 0) for aa in seq[:MAX_LEN]]\n",
    "        # Padding (ƒëi·ªÅn s·ªë 0 v√†o ƒëu√¥i cho ƒë·ªß ƒë·ªô d√†i MAX_LEN)\n",
    "        if len(encoded_seq) < MAX_LEN:\n",
    "            encoded_seq += [0] * (MAX_LEN - len(encoded_seq))\n",
    "            \n",
    "        sequences.append(encoded_seq)\n",
    "        ids.append(pid)\n",
    "        \n",
    "    return np.array(sequences, dtype=np.int32), ids\n",
    "\n",
    "# --- LOAD D·ªÆ LI·ªÜU ---\n",
    "# 1. Load Sequence\n",
    "X_train_seq, train_ids = get_one_hot_data(TRAIN_FASTA)\n",
    "\n",
    "# 2. Load Labels (Y) - Top 1500 nh√£n ph·ªï bi·∫øn\n",
    "print(\"‚è≥ Processing Labels...\")\n",
    "train_terms = pd.read_csv(TRAIN_TERMS, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "TOP_N = 1500\n",
    "top_terms = train_terms[\"term\"].value_counts().head(TOP_N).index.tolist()\n",
    "term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "\n",
    "Y_train = np.zeros((len(train_ids), TOP_N), dtype=np.float32)\n",
    "id_map = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "# Fill Y matrix\n",
    "grouped = train_terms[train_terms[\"term\"].isin(top_terms)].groupby(\"EntryID\")[\"term\"].apply(list)\n",
    "for pid, terms in tqdm(grouped.items()):\n",
    "    if pid in id_map:\n",
    "        indices = [term_to_idx[t] for t in terms]\n",
    "        Y_train[id_map[pid], indices] = 1.0\n",
    "\n",
    "print(f\"‚úÖ Data Ready: X={X_train_seq.shape}, Y={Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876edb0d-3faa-4332-927f-fd25e8f18ecc",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f01b2f5-f22d-4a02-bac2-d1b4de5ec84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model CNN (DeepGOPlus style) ƒë√£ s·∫µn s√†ng chi·∫øn ƒë·∫•u!\n"
     ]
    }
   ],
   "source": [
    "class DeepGOPlus(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size=21, embedding_dim=128, num_filters=512, kernel_sizes=[8, 16, 24, 32]):\n",
    "        super(DeepGOPlus, self).__init__()\n",
    "        \n",
    "        # L·ªõp Embedding: Bi·∫øn s·ªë nguy√™n (1, 2...) th√†nh vector d√†y ƒë·∫∑c\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # C√°c l·ªõp Convolution song song (Multi-kernel)\n",
    "        # M·ªói kernel size s·∫Ω b·∫Øt c√°c motif c√≥ ƒë·ªô d√†i kh√°c nhau\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # L·ªõp ph√¢n lo·∫°i cu·ªëi c√πng\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "        self.dropout = nn.Dropout(0.5) # Ch·ªëng h·ªçc v·∫πt\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len]\n",
    "        x = self.embedding(x)           # -> [batch, seq_len, emb_dim]\n",
    "        x = x.permute(0, 2, 1)          # -> [batch, emb_dim, seq_len] (Pytorch Conv1d c·∫ßn channel ·ªü gi·ªØa)\n",
    "        \n",
    "        # Ch·∫°y qua t·ª´ng b·ªô l·ªçc Conv -> ReLU -> MaxPool\n",
    "        outs = []\n",
    "        for conv in self.convs:\n",
    "            out = conv(x)               # Convolution\n",
    "            out = torch.relu(out)       # Activation\n",
    "            out, _ = torch.max(out, dim=2) # Global Max Pooling (L·∫•y t√≠n hi·ªáu m·∫°nh nh·∫•t c·ªßa motif)\n",
    "            outs.append(out)\n",
    "            \n",
    "        # N·ªëi c√°c ƒë·∫∑c tr∆∞ng l·∫°i\n",
    "        out = torch.cat(outs, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out # Tr·∫£ v·ªÅ Logits (ch∆∞a qua Sigmoid)\n",
    "\n",
    "# Kh·ªüi t·∫°o\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DeepGOPlus(num_classes=TOP_N).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"‚úÖ Model CNN (DeepGOPlus style) ƒë√£ s·∫µn s√†ng chi·∫øn ƒë·∫•u!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b51ad-acde-4381-a17d-aa8d7cfefabb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ef2ec4-e8c2-437e-ab34-c259eab0c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu Train CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0173 | Val Loss = 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0156 | Val Loss = 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0153 | Val Loss = 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0150 | Val Loss = 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0147 | Val Loss = 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0144 | Val Loss = 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0141 | Val Loss = 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0138 | Val Loss = 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0135 | Val Loss = 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0132 | Val Loss = 0.0149\n",
      "‚úÖ Train xong & ƒê√£ l∆∞u model!\n"
     ]
    }
   ],
   "source": [
    "# Chia t·∫≠p train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_train_seq, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Loader\n",
    "train_ds = data.TensorDataset(torch.from_numpy(X_tr).long(), torch.from_numpy(Y_tr).float())\n",
    "val_ds = data.TensorDataset(torch.from_numpy(X_val).long(), torch.from_numpy(Y_val).float())\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = data.DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu Train CNN...\")\n",
    "for epoch in range(10): # Train nhanh 10 epoch\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for bx, by in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(bx)\n",
    "        loss = criterion(outputs, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in val_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            val_loss += criterion(model(bx), by).item()\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {total_loss/len(train_loader):.4f} | Val Loss = {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# L∆∞u model\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"‚úÖ Train xong & ƒê√£ l∆∞u model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5ed6ec-f9e0-49d4-8b3a-aaca0d32a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang m√£ h√≥a One-Hot cho t·∫≠p Test (s·∫Ω m·∫•t 1-2 ph√∫t)...\n",
      "‚è≥ Encoding /workspace/data/Test/testsuperset.fasta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224309it [00:04, 48922.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load 224309 protein test.\n",
      "üöÄ ƒêang ch·∫°y model CNN tr√™n t·∫≠p Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1753/1753 [00:18<00:00, 93.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ ƒêang ghi k·∫øt qu·∫£ xu·ªëng submission_cnn_raw.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:07<00:00, 28310.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c√≥ file d·ª± ƒëo√°n th√¥ c·ªßa CNN!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "TEST_FASTA = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "OUTPUT_CNN_RAW = \"submission_cnn_raw.tsv\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 1. Encode t·∫≠p Test (D√πng l·∫°i h√†m get_one_hot_data c≈©)\n",
    "print(\"‚è≥ ƒêang m√£ h√≥a One-Hot cho t·∫≠p Test (s·∫Ω m·∫•t 1-2 ph√∫t)...\")\n",
    "# H√†m get_one_hot_data ƒë√£ khai b√°o ·ªü cell tr∆∞·ªõc\n",
    "X_test_seq, test_ids = get_one_hot_data(TEST_FASTA)\n",
    "\n",
    "# Chuy·ªÉn sang Tensor\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test_seq).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ load {len(test_ids)} protein test.\")\n",
    "\n",
    "# 2. D·ª± ƒëo√°n (Inference)\n",
    "print(\"üöÄ ƒêang ch·∫°y model CNN tr√™n t·∫≠p Test...\")\n",
    "model.eval()\n",
    "output_lines = []\n",
    "# Danh s√°ch nh√£n (top_terms) ƒë√£ c√≥ t·ª´ cell tr∆∞·ªõc\n",
    "\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bx in tqdm(test_loader):\n",
    "        bx = bx[0].to(device)\n",
    "        logits = model(bx)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "\n",
    "# G·ªôp t·∫•t c·∫£ batch l·∫°i\n",
    "all_probs = np.vstack(all_probs)\n",
    "\n",
    "# 3. Ghi file (L·∫•y Top 50 ƒë·ªÉ nh·∫π file)\n",
    "print(f\"üíæ ƒêang ghi k·∫øt qu·∫£ xu·ªëng {OUTPUT_CNN_RAW}...\")\n",
    "for i, pid in enumerate(tqdm(test_ids)):\n",
    "    # L·∫•y x√°c su·∫•t c·ªßa protein i\n",
    "    prob_row = all_probs[i]\n",
    "    \n",
    "    # L·ªçc Top 50 nh√£n cao nh·∫•t\n",
    "    top_indices = np.argsort(prob_row)[-50:]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        score = prob_row[idx]\n",
    "        # Ch·ªâ l·∫•y n·∫øu score > 0.01\n",
    "        if score > 0.01:\n",
    "            term = top_terms[idx] # L·∫•y t√™n GO Term\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "with open(OUTPUT_CNN_RAW, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"‚úÖ ƒê√£ c√≥ file d·ª± ƒëo√°n th√¥ c·ªßa CNN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f9db08-09f1-41b7-9d5a-b509b4867b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading OBO: /workspace/data/Train/go-basic.obo\n",
      "üöÄ Propagating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224305/224305 [00:12<00:00, 17964.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ xong file CNN chu·∫©n: submission_cnn_propagated.tsv\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH ---\n",
    "INPUT_FILE = \"submission_cnn_raw.tsv\"\n",
    "OUTPUT_FILE = \"submission_cnn_propagated.tsv\"\n",
    "\n",
    "# (ƒê·∫£m b·∫£o ƒë√£ c√†i obonet: pip install obonet networkx)\n",
    "import obonet\n",
    "import networkx\n",
    "import os\n",
    "\n",
    "# Load OBO\n",
    "OBO_PATH = \"/workspace/data/go-basic.obo\"\n",
    "if not os.path.exists(OBO_PATH):\n",
    "    OBO_PATH = \"/workspace/data/Train/go-basic.obo\"\n",
    "\n",
    "print(f\"üìñ Reading OBO: {OBO_PATH}\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "ancestors_map = {}\n",
    "for node in graph.nodes():\n",
    "    try:\n",
    "        ancestors_map[node] = networkx.descendants(graph, node)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ƒê·ªçc file & Propagate\n",
    "print(\"üöÄ Propagating scores...\")\n",
    "preds = {}\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in f:\n",
    "        p, t, s = line.strip().split('\\t')\n",
    "        if p not in preds: preds[p] = {}\n",
    "        preds[p][t] = float(s)\n",
    "\n",
    "final_lines = []\n",
    "for pid, scores in tqdm(preds.items()):\n",
    "    new_scores = scores.copy()\n",
    "    for term, score in scores.items():\n",
    "        if term in ancestors_map:\n",
    "            for parent in ancestors_map[term]:\n",
    "                new_scores[parent] = max(new_scores.get(parent, 0.0), score)\n",
    "    \n",
    "    # Ghi l·∫°i (Top 70)\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n",
    "    for term, score in sorted_terms:\n",
    "        if score > 0.01:\n",
    "            final_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(final_lines))\n",
    "print(f\"‚úÖ ƒê√£ xong file CNN chu·∫©n: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75eee72-5237-4811-b419-0e41b71795da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang load 2 file ƒë·ªÉ h·ª£p th·ªÉ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading KNN: 14386016it [00:08, 1712368.83it/s]\n",
      "Loading CNN: 14009920it [00:08, 1708294.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FUSION: Mixing KNN (Sequence Sim) & CNN (Motif)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23460696/23460696 [00:27<00:00, 859221.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ XONG! File 'submission_FINAL_FUSION_0.35.tsv' l√† ni·ªÅm hy v·ªçng cu·ªëi c√πng.\n",
      "üëâ H√£y n·ªôp file n√†y. S·ª± k·∫øt h·ª£p gi·ªØa Global (ESM2) v√† Local (CNN) th∆∞·ªùng ƒë·∫©y ƒëi·ªÉm l√™n r·∫•t cao!\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH ---\n",
    "# 1. File KNN x·ªãn nh·∫•t c·ªßa b·∫°n (File ƒë·∫°t 0.256)\n",
    "# H√£y thay t√™n file n√†y b·∫±ng t√™n file t·ªët nh·∫•t b·∫°n ƒëang c√≥!\n",
    "FILE_KNN = \"submission_level11_IA_official.tsv\" \n",
    "# N·∫øu b·∫°n l∆∞u t√™n kh√°c (vd submission_v11_B_bold.tsv) th√¨ s·ª≠a l·∫°i nh√©!\n",
    "\n",
    "# 2. File CNN v·ª´a t·∫°o\n",
    "FILE_CNN = \"submission_cnn_propagated.tsv\"\n",
    "\n",
    "# 3. File n·ªôp cu·ªëi c√πng\n",
    "OUTPUT_ENSEMBLE = \"submission_FINAL_FUSION_0.35.tsv\"\n",
    "\n",
    "# Tr·ªçng s·ªë (Weights)\n",
    "W_KNN = 0.60\n",
    "W_CNN = 0.40\n",
    "\n",
    "print(\"‚è≥ ƒêang load 2 file ƒë·ªÉ h·ª£p th·ªÉ...\")\n",
    "preds_knn = {}\n",
    "with open(FILE_KNN) as f:\n",
    "    for line in tqdm(f, desc=\"Loading KNN\"):\n",
    "        p, t, s = line.strip().split('\\t')\n",
    "        preds_knn[(p, t)] = float(s)\n",
    "\n",
    "preds_cnn = {}\n",
    "with open(FILE_CNN) as f:\n",
    "    for line in tqdm(f, desc=\"Loading CNN\"):\n",
    "        p, t, s = line.strip().split('\\t')\n",
    "        preds_cnn[(p, t)] = float(s)\n",
    "\n",
    "# Tr·ªôn\n",
    "print(\"üöÄ FUSION: Mixing KNN (Sequence Sim) & CNN (Motif)...\")\n",
    "all_keys = set(preds_knn.keys()) | set(preds_cnn.keys())\n",
    "output_lines = []\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    s1 = preds_knn.get(key, 0.0)\n",
    "    s2 = preds_cnn.get(key, 0.0)\n",
    "    \n",
    "    # Weighted Average\n",
    "    final_score = (s1 * W_KNN) + (s2 * W_CNN)\n",
    "    \n",
    "    if final_score > 0.01:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "with open(OUTPUT_ENSEMBLE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"üéâ XONG! File '{OUTPUT_ENSEMBLE}' l√† ni·ªÅm hy v·ªçng cu·ªëi c√πng.\")\n",
    "print(\"üëâ H√£y n·ªôp file n√†y. S·ª± k·∫øt h·ª£p gi·ªØa Global (ESM2) v√† Local (CNN) th∆∞·ªùng ƒë·∫©y ƒëi·ªÉm l√™n r·∫•t cao!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9fb8c-f740-4a20-bd28-c437e55089e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
