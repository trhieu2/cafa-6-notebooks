{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb88d2c5-f7b1-49cd-9e45-df552f867a80",
   "metadata": {},
   "source": [
    "### Drawback of traditional ML approach: Kh√¥ng gi·ªØ ƒë∆∞·ª£c order meaning c·ªßa chu·ªói axit amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac6754-31c2-4d28-9654-8867e3b45e60",
   "metadata": {},
   "source": [
    "#### Sol: s·ª≠ d·ª•ng m√¥ h√¨nh esm2 650M tham s·ªë ƒë·ªÉ embedding c√°c chu·ªói acid amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa35b0-af98-41c1-89bc-6e426b6dd599",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69266e5f-6de0-4045-89bc-4f204724623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c681d99-6943-4c20-9dea-c2983627f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87397ece-1f18-4462-8662-9444fcc2fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/esm2_t33_650M_UR50D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-32): 33 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval() #read-only to save VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fc7c8-dabb-4934-a152-6e57c5befb31",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f609ee-33d4-4bbf-b76a-669d20fe9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(fasta_path, save_name, batch_size=8, limit=None):\n",
    "    \"\"\"\n",
    "    Read fasta file -> Run through ESM2 -> output: .npy\n",
    "    save_name: output file\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    sequences = []\n",
    "\n",
    "    print(f\"Reading file: {fasta_path}\")\n",
    "    for i, record in enumerate(SeqIO.parse(fasta_path, \"fasta\")):\n",
    "        if limit and i >= limit: break\n",
    "\n",
    "        #clean id\n",
    "        pid = str(record.id)\n",
    "        if \"|\" in pid:\n",
    "            pid = pid.split(\"|\")[1]\n",
    "\n",
    "        ids.append(pid)\n",
    "        #esm2 limit 1024 token\n",
    "        sequences.append(str(record.seq)[:1022])\n",
    "\n",
    "    print(f\"{len(sequences)} Proteins\")\n",
    "\n",
    "    #batching\n",
    "    embeddings = []\n",
    "    print(\"Creating embeddings...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        batch_seqs = sequences[i : i + batch_size]\n",
    "\n",
    "        #tokenize\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #mean pooling\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "        sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "    final_embeddings = np.vstack(embeddings)\n",
    "\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}.npy\", final_embeddings)\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}_ids.npy\", ids)\n",
    "\n",
    "    return ids, final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9cc34-a585-4965-924b-0e4bfa46cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "\n",
    "train_ids, X_train = extract_embeddings(\n",
    "    train_fasta, \n",
    "    save_name=\"train_650M\", \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516fd8c-8525-4a68-aaef-2b7898aa4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#prepare labels\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set)]\n",
    "\n",
    "top_n = 1500\n",
    "top_terms = train_terms_filtered[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "\n",
    "Y_matrix = train_terms_filtered[train_terms_filtered[\"term\"].isin(top_terms)] \\\n",
    "            .pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b21e0-2840-473a-9924-d77f53b286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c7e45-86be-492c-be7a-d371b03853f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df688e-8fca-4eae-af99-e1c77f6b01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_val = clf.predict(X_val)\n",
    "score = f1_score(Y_val, Y_pred_val, average='micro')\n",
    "print(f\"Local F1-Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073505a7-5e08-4218-9324-aa702bb1858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell ch·∫©n ƒëo√°n l·ªói ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 1. Ki·ªÉm tra xem m√¥ h√¨nh c√≥ d·ª± ƒëo√°n ra c√°i g√¨ kh√¥ng?\n",
    "print(f\"T·ªïng s·ªë m·∫´u trong t·∫≠p Val: {Y_val.shape[0]}\")\n",
    "print(f\"T·ªïng s·ªë nh√£n c·∫ßn d·ª± ƒëo√°n: {Y_val.shape[0] * Y_val.shape[1]}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (Th·ª±c t·∫ø): {Y_val.sum()}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (M√¥ h√¨nh d·ª± ƒëo√°n): {Y_pred_val.sum()}\")\n",
    "\n",
    "# 2. N·∫øu s·ªë d·ª± ƒëo√°n qu√° th·∫•p (g·∫ßn b·∫±ng 0), ta c·∫ßn h·∫° ng∆∞·ª°ng (Threshold)\n",
    "print(\"\\n--- Th·ª≠ ch·ªânh ng∆∞·ª°ng th·ªß c√¥ng ---\")\n",
    "# L·∫•y ƒëi·ªÉm s·ªë th√¥ thay v√¨ nh√£n c·ª©ng 0/1\n",
    "decision_scores = clf.decision_function(X_val) \n",
    "\n",
    "# Th·ª≠ c√°c ng∆∞·ª°ng kh√°c nhau\n",
    "for thr in [0, -0.5, -1.0]: # Ridge score c√≥ th·ªÉ √¢m\n",
    "    y_pred_new = (decision_scores > thr).astype(int)\n",
    "    new_f1 = f1_score(Y_val, y_pred_new, average='micro')\n",
    "    print(f\"Ng∆∞·ª°ng {thr}: F1-Score = {new_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6d097d-5ca1-4459-a286-95445364e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [1:46:48<00:00, 47.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "FASTA_PATH = \"/workspace/data/Test/testsuperset.fasta\" \n",
    "SAVE_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "CHUNK_SIZE = 5000  \n",
    "BATCH_SIZE = 8    \n",
    "\n",
    "# --- SETUP ---\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "def process_and_save(seqs, ids, part_idx):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(seqs), BATCH_SIZE):\n",
    "        batch_seqs = seqs[i : i + BATCH_SIZE]\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "            sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "    \n",
    "    final_emb = np.vstack(embeddings)\n",
    "    np.save(f\"{SAVE_DIR}/test_part_{part_idx}.npy\", final_emb)\n",
    "    np.save(f\"{SAVE_DIR}/test_ids_{part_idx}.npy\", ids)\n",
    "\n",
    "sequences = []\n",
    "ids = []\n",
    "part_counter = 0\n",
    "\n",
    "pbar = tqdm(total=224309) \n",
    "\n",
    "for record in SeqIO.parse(FASTA_PATH, \"fasta\"):\n",
    "    save_path_check = f\"{SAVE_DIR}/test_part_{part_counter}.npy\"\n",
    "    \n",
    "    if os.path.exists(save_path_check):\n",
    "        sequences.append(1) \n",
    "        if len(sequences) >= CHUNK_SIZE:\n",
    "            sequences = [] \n",
    "            ids = []\n",
    "            part_counter += 1\n",
    "            pbar.update(CHUNK_SIZE)\n",
    "        continue\n",
    "\n",
    "    pid = str(record.id).split(\"|\")[1] if \"|\" in str(record.id) else str(record.id)\n",
    "    ids.append(pid)\n",
    "    sequences.append(str(record.seq)[:1022])\n",
    "    pbar.update(1)\n",
    "    \n",
    "    if len(sequences) >= CHUNK_SIZE: \n",
    "        process_and_save(sequences, ids, part_counter)\n",
    "        part_counter += 1\n",
    "        \n",
    "        sequences = []\n",
    "        ids = []\n",
    "        gc.collect()\n",
    "\n",
    "if len(sequences) > 0:\n",
    "    process_and_save(sequences, ids, part_counter)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ade8a3-f0ab-48ed-ba9a-2951ef9a5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\" \n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_CHUNKS_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE = \"submission_level4_FINAL_fixed.tsv\"\n",
    "\n",
    "# Re-training\n",
    "print(\"Loading data and training model...\")\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "top_n = 1500\n",
    "top_terms = train_terms[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set) & train_terms[\"term\"].isin(top_terms)]\n",
    "\n",
    "Y_matrix = train_terms_filtered.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)\n",
    "terms_columns = Y_train.columns \n",
    "\n",
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "del X_train, Y_train, train_terms, train_terms_filtered, Y_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"Outputting (Top K)...\")\n",
    "\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "TOP_K = 50   \n",
    "THRESHOLD = 0.01 #increased threshold\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    X_chunk = np.load(f_path)\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_chunk = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    #predicting\n",
    "    decision_scores = clf.decision_function(X_chunk)\n",
    "    probs = 1 / (1 + np.exp(-decision_scores))\n",
    "    \n",
    "    for i, pid in enumerate(ids_chunk):\n",
    "        prob_row = probs[i]\n",
    "        \n",
    "        # 1. thresholding\n",
    "        mask = prob_row > THRESHOLD\n",
    "        if not np.any(mask):\n",
    "            indices = np.argsort(prob_row)[-5:]\n",
    "        else:\n",
    "            candidates = np.where(mask)[0]\n",
    "            \n",
    "            #2: only select the top 50\n",
    "            if len(candidates) > TOP_K:\n",
    "                # get candidate scores\n",
    "                cand_probs = prob_row[candidates]\n",
    "                # sort for top k\n",
    "                top_k_local_idx = np.argsort(cand_probs)[-TOP_K:]\n",
    "                indices = candidates[top_k_local_idx]\n",
    "            else:\n",
    "                indices = candidates\n",
    "            \n",
    "        for idx in indices:\n",
    "            term = terms_columns[idx]\n",
    "            score = prob_row[idx]\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "            \n",
    "    del X_chunk, ids_chunk, decision_scores, probs\n",
    "    gc.collect()\n",
    "\n",
    "# Output file\n",
    "print(f\"üíæ Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a3be4-23ae-472b-a210-f280f2013ff1",
   "metadata": {},
   "source": [
    "#### Score: 0.192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af3504-0f41-4ad2-9552-fa926369c9a6",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998d396-61f8-462b-9e2b-5163034c552a",
   "metadata": {},
   "source": [
    "### GO Hierarchy: Ridge classifier ƒëang h·ªçc c√°c nh√£n 1 c√°ch ƒë·ªôc l·∫≠p, nh√£n con c√≥ th·ªÉ c√≥ score cao, nh∆∞ng nh·ªØng nh√£n cha chung chung th√¨ score l·∫°i th·∫•p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0556a-def4-4de0-bbe2-5526eac4b75d",
   "metadata": {},
   "source": [
    "### Sol: Ensemble: Mix v·ªõi naive approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9861fdff-8d1b-4719-983a-02e2947cbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ESM2 output file....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:07, 1548396.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Naive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10093905it [00:05, 1690414.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17589919/17589919 [00:20<00:00, 844235.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /workspace/notebooks/submission_ensemble_boosted.tsv\n",
      "Finisehd.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEVEL4_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "NAIVE_FILE = \"/workspace/notebooks/submission_naive.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_ensemble_boosted.tsv\"\n",
    "\n",
    "print(\"Reading ESM2 output file....\")\n",
    "preds_l4 = {}\n",
    "\n",
    "try:\n",
    "    with open(LEVEL4_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            # Key : (ProteinID, GO_Term)\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_l4[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {LEVEL4_FILE}.\")\n",
    "    raise\n",
    "\n",
    "print(\"Reading file Naive...\")\n",
    "preds_naive = {}\n",
    "try:\n",
    "    with open(NAIVE_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_naive[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {NAIVE_FILE}\")\n",
    "    raise\n",
    "\n",
    "print(\"Ensembling...\")\n",
    "\n",
    "#Select all id-term pairs in 2 files\n",
    "all_keys = set(preds_l4.keys()) | set(preds_naive.keys())\n",
    "output_lines = []\n",
    "\n",
    "W_L4 = 0.6\n",
    "W_NAIVE = 0.4\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    # L·∫•y ƒëi·ªÉm s·ªë, n·∫øu file n√†o kh√¥ng c√≥ th√¨ coi l√† 0\n",
    "    score_l4 = preds_l4.get(key, 0.0)\n",
    "    score_naive = preds_naive.get(key, 0.0)\n",
    "    \n",
    "    # C√¥ng th·ª©c c·ªông g·ªôp\n",
    "    final_score = (score_l4 * W_L4) + (score_naive * W_NAIVE)\n",
    "    \n",
    "    # Ch·ªâ ghi nh·ªØng d√≤ng c√≥ ƒëi·ªÉm s·ªë > 0.001 ƒë·ªÉ file ƒë·ª° n·∫∑ng\n",
    "    if final_score > 0.001:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "# Ghi ra file\n",
    "print(f\"Saving: {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"Finisehd.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fccd1-0fb3-4f0a-bb2d-8f2b5f791e3e",
   "metadata": {},
   "source": [
    "### Ensemble ESM2 + BLAST/Diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a522344-5b1a-4176-89c3-3c581bc841e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnign BLAST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diamond v2.1.8.162 (C) Max Planck Society for the Advancement of Science, Benjamin Buchfink, University of Tuebingen\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "Please cite: http://dx.doi.org/10.1038/s41592-021-01101-x Nature Methods (2021)\n",
      "\n",
      "#CPU threads: 32\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Temporary directory: /workspace/notebooks\n",
      "Percentage range of top alignment score to report hits: 1\n",
      "Opening the database...  [0.06s]\n",
      "Database: /workspace/data/Traintrain_data.dmnd (type: Diamond database, sequences: 82404, letters: 43327058)\n",
      "Block size = 2000000000\n",
      "Opening the input file...  [0.035s]\n",
      "Opening the output file...  [0s]\n",
      "Loading query sequences...  [0.215s]\n",
      "Masking queries...  [0.141s]\n",
      "Algorithm: Double-indexed\n",
      "Building query histograms...  [0.513s]\n",
      "Seeking in database...  [0s]\n",
      "Loading reference sequences...  [0.051s]\n",
      "Masking reference...  [0.061s]\n",
      "Initializing temporary storage...  [0s]\n",
      "Building reference histograms...  [0.22s]\n",
      "Allocating buffers...  [0.002s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.038s]\n",
      "Computing hash join...  [0.037s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.431s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.36s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.338s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.021s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.039s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.313s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.365s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.338s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.35s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.041s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.374s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.045s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.335s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.022s]\n",
      "Building query seed array...  [0.049s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.318s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.309s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.344s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.317s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.036s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.051s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.325s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.318s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.349s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.341s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.328s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.34s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.324s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.343s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.332s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.315s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.335s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.02s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.342s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.032s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.313s]\n",
      "Deallocating memory...  [0s]\n",
      "Deallocating buffers...  [0.019s]\n",
      "Clearing query masking...  [0.006s]\n",
      "Computing alignments... Loading trace points...  [0.772s]\n",
      "Sorting trace points...  [0.214s]\n",
      "Computing alignments...  [9.531s]\n",
      "Deallocating buffers...  [0.061s]\n",
      "Loading trace points...  [0s]\n",
      " [10.659s]\n",
      "Deallocating reference...  [0.002s]\n",
      "Loading reference sequences...  [0s]\n",
      "Deallocating buffers...  [0s]\n",
      "Deallocating queries...  [0.005s]\n",
      "Loading query sequences...  [0s]\n",
      "Closing the input file...  [0s]\n",
      "Closing the output file...  [0s]\n",
      "Closing the database...  [0.003s]\n",
      "Cleaning up...  [0s]\n",
      "Total time = 40.168s\n",
      "Reported 262530 pairwise alignments, 262530 HSPs.\n",
      "204232 queries aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished blasting\n",
      "Handling output of BLAST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262530/262530 [00:03<00:00, 72338.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Level 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:07, 1560616.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11968960/11968960 [00:12<00:00, 950665.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /workspace/notebooks/submission_hybrid_blast_esm2.tsv...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEVEL4_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "\n",
    "DIAMOND_BIN = \"/usr/bin/diamond\"\n",
    "TRAIN_FASTA = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "TEST_FASTA = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "DB_PATH = \"/workspace/data/Traintrain_data.dmnd\"\n",
    "BLAST_RESULT = \"/workspace/notebooks/diamond_results.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_hybrid_blast_esm2.tsv\"\n",
    "\n",
    "print(\"Runnign BLAST...\")\n",
    "\n",
    "#create db\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"ƒêang t·∫°o database...\")\n",
    "    os.system(f\"{DIAMOND_BIN} makedb --in {TRAIN_FASTA} -d {DB_PATH} --quiet\")\n",
    "\n",
    "#alignment\n",
    "cmd = f\"{DIAMOND_BIN} blastp -d {DB_PATH} -q {TEST_FASTA} -o {BLAST_RESULT} --sensitive --top 1 -f 6 qseqid sseqid pident\"\n",
    "os.system(cmd)\n",
    "print(\"Finished blasting\")\n",
    "\n",
    "print(\"Handling output of BLAST...\")\n",
    "blast_preds = {}\n",
    "\n",
    "# Load train terms ƒë·ªÉ map nh√£n\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "train_terms_grouped = train_terms.groupby(\"EntryID\")[\"term\"].apply(list).to_dict()\n",
    "\n",
    "# ƒê·ªçc k·∫øt qu·∫£ BLAST\n",
    "df_blast = pd.read_csv(BLAST_RESULT, sep=\"\\t\", names=[\"test_id\", \"train_id\", \"pident\"])\n",
    "\n",
    "for _, row in tqdm(df_blast.iterrows(), total=len(df_blast)):\n",
    "    # Clean ID (S·ª≠a l·ªói ID b·ªã d√≠nh sp|...)\n",
    "    test_id = str(row['test_id']).split(\"|\")[1] if \"|\" in str(row['test_id']) else str(row['test_id'])\n",
    "    train_id = str(row['train_id']).split(\"|\")[1] if \"|\" in str(row['train_id']) else str(row['train_id'])\n",
    "    \n",
    "    score = row['pident'] / 100.0\n",
    "    \n",
    "    # Ch·ªâ l·∫•y nh·ªØng th·∫±ng gi·ªëng nhau > 30% (Ng∆∞·ª°ng an to√†n)\n",
    "    if score < 0.3: continue\n",
    "    \n",
    "    if train_id in train_terms_grouped:\n",
    "        for term in train_terms_grouped[train_id]:\n",
    "            key = (test_id, term)\n",
    "            # BLAST r·∫•t uy t√≠n, n√™n gi·ªØ nguy√™n score cao\n",
    "            blast_preds[key] = score\n",
    "\n",
    "print(\"Reading file Level 4...\")\n",
    "esm_preds = {}\n",
    "try:\n",
    "    with open(LEVEL4_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            key = (parts[0], parts[1])\n",
    "            esm_preds[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {LEVEL4_FILE}.\")\n",
    "    raise\n",
    "\n",
    "print(\"Mixing...\")\n",
    "\n",
    "all_keys = set(blast_preds.keys()) | set(esm_preds.keys())\n",
    "output_lines = []\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    s_blast = blast_preds.get(key, 0.0)\n",
    "    s_esm = esm_preds.get(key, 0.0)\n",
    "    \n",
    "    # CHI·∫æN THU·∫¨T QUAN TR·ªåNG:\n",
    "    # N·∫øu BLAST t√¨m th·∫•y -> Tin BLAST (v√¨ n√≥ so kh·ªõp ch√≠nh x√°c)\n",
    "    # N·∫øu BLAST kh√¥ng th·∫•y -> Tin ESM (v√¨ n√≥ suy lu·∫≠n t·ªët)\n",
    "    # => L·∫•y MAX\n",
    "    final_score = max(s_blast, s_esm)\n",
    "    \n",
    "    output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "#GHI FILE\n",
    "print(f\"Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6ff76-d373-4e12-8fa9-9d35ffe443cd",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558f6ac7-0685-4bab-8850-ea30b7db9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting obonet\n",
      "  Downloading obonet-1.1.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (3.3)\n",
      "Downloading obonet-1.1.1-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: obonet\n",
      "Successfully installed obonet-1.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install obonet networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294bd6ab-f94a-414d-a360-34449af1c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_level4_propagated.tsv\"\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a5e59-213b-4ba7-9958-eeb0f7cfca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªçc c√¢y ph·∫£ h·ªá...\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒê·ªçc c√¢y ph·∫£ h·ªá...\")\n",
    "graph = obonet.read_obo(OBO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0300ecaf-ed1b-421c-a5ac-99f6023abff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X√¢y map quan h·ªá cha-con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 132343.17it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"X√¢y map quan h·ªá cha-con...\")\n",
    "ancestors_map = {}\n",
    "for node in tqdm(graph.nodes()):\n",
    "    # networkx tr·∫£ v·ªÅ danh s√°ch t·ªï ti√™n\n",
    "    try:\n",
    "        ancestors = networkx.descendants(graph, node) # Trong obonet, chi·ªÅu m≈©i t√™n ng∆∞·ª£c (Con -> Cha)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feda38f8-9b19-43d5-8aa2-1b16cf5de9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc /workspace/notebooks/submission_level4_FINAL_fixed.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:04, 2284548.44it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang ƒë·ªçc {INPUT_FILE}...\")\n",
    "# ƒê·ªçc v√†o Dict: {ProteinID: {Term: Score}}\n",
    "preds = {}\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        if pid not in preds: preds[pid] = {}\n",
    "        preds[pid][term] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99afb86c-c69c-4f47-b9ca-5d5e81f17f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lan truy·ªÅn ng∆∞·ª£c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:22<00:00, 9829.63it/s] \n"
     ]
    }
   ],
   "source": [
    "print(\"Lan truy·ªÅn ng∆∞·ª£c...\")\n",
    "final_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # term_scores l√† dict {Term: Score g·ªëc}\n",
    "    # new_scores s·∫Ω ch·ª©a c·∫£ ƒëi·ªÉm c·ªßa cha √¥ng\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    for term, score in term_scores.items():\n",
    "        # L·∫•y danh s√°ch cha √¥ng c·ªßa term n√†y\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # Quy t·∫Øc: ƒêi·ªÉm c·ªßa cha = MAX(ƒêi·ªÉm cha c≈©, ƒêi·ªÉm c·ªßa con)\n",
    "                current_parent_score = new_scores.get(parent, 0.0)\n",
    "                new_scores[parent] = max(current_parent_score, score)\n",
    "    \n",
    "    # Ghi ra k·∫øt qu·∫£ (L·∫°i ph·∫£i l·ªçc Top K v√¨ gi·ªù n√≥ ph√¨nh to ra)\n",
    "    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # L·∫•y Top 70 (tƒÉng l√™n ch√∫t v√¨ gi·ªù c√≥ c·∫£ cha √¥ng)\n",
    "    for term, score in sorted_terms[:70]:\n",
    "        final_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e773adc4-a1ea-4912-ba82-05b61497296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /workspace/notebooks/submission_level4_propagated.tsv...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(final_lines))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecdb47-8b91-4fa1-ae90-77d46dfe8faa",
   "metadata": {},
   "source": [
    "### S·ª≠ d·ª•ng KNN clf tr√™n embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91df6974-1b5f-4d07-9884-9aff772279d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\"\n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_CHUNKS_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE = \"submission_knn_esm2.tsv\"\n",
    "TOP_K = 5  # L·∫•y 5 ng∆∞·ªùi h√†ng x√≥m gi·ªëng nh·∫•t\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f43d68d-8828-4626-b65b-f6e4c935c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Train Embeddings...\n",
      "Loading labels...\n",
      "Train xong tren GPU!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Train Embeddings...\")\n",
    "# Load vector\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "X_train = torch.from_numpy(X_train).to(device)\n",
    "\n",
    "# Chu·∫©n h√≥a vector v·ªÅ ƒë∆°n v·ªã (ƒë·ªÉ t√≠nh Cosine Similarity nhanh b·∫±ng ph√©p nh√¢n ma tr·∫≠n)\n",
    "# C√¥ng th·ª©c: v = v / |v|\n",
    "norm = X_train.norm(p=2, dim=1, keepdim=True)\n",
    "X_train = X_train.div(norm)\n",
    "\n",
    "# Load ID v√† Map Nh√£n\n",
    "print(\"Loading labels...\")\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "# Gom nh√≥m: TrainID -> Set(Terms)\n",
    "train_labels_map = train_terms.groupby(\"EntryID\")[\"term\"].apply(set).to_dict()\n",
    "\n",
    "# Map Index -> ID (ƒë·ªÉ truy xu·∫•t nhanh t·ª´ k·∫øt qu·∫£ KNN)\n",
    "idx_to_trainid = {i: pid for i, pid in enumerate(train_ids)}\n",
    "\n",
    "print(\"Train xong tren GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e6e9ad-aca9-4109-b720-f37a93f1ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu ch·∫°y KNN (T√¨m h√†ng x√≥m)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:05<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu ch·∫°y KNN (T√¨m h√†ng x√≥m)...\")\n",
    "\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    # 1. Load 1 c·ª•c Test l√™n GPU\n",
    "    X_test_np = np.load(f_path)\n",
    "    X_test = torch.from_numpy(X_test_np).to(device)\n",
    "    \n",
    "    # Chu·∫©n h√≥a Test\n",
    "    norm_test = X_test.norm(p=2, dim=1, keepdim=True)\n",
    "    X_test = X_test.div(norm_test)\n",
    "    \n",
    "    # Load ID Test t∆∞∆°ng ·ª©ng\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_test = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    # 2. T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng (Matrix Multiplication)\n",
    "    # [Batch, 1280] x [1280, All_Train] = [Batch, All_Train]\n",
    "    # ƒê√¢y l√† b∆∞·ªõc n·∫∑ng nh·∫•t, nh∆∞ng GPU x·ª≠ l√Ω t·ªët\n",
    "    sim_matrix = torch.mm(X_test, X_train.t())\n",
    "    \n",
    "    # 3. L·∫•y Top K h√†ng x√≥m\n",
    "    # values: ƒë·ªô t∆∞∆°ng ƒë·ªìng (score), indices: v·ªã tr√≠ c·ªßa h√†ng x√≥m\n",
    "    topk_values, topk_indices = torch.topk(sim_matrix, k=TOP_K, dim=1)\n",
    "    \n",
    "    # Chuy·ªÉn v·ªÅ CPU ƒë·ªÉ x·ª≠ l√Ω logic g√°n nh√£n (Python x·ª≠ l√Ω dict nhanh h∆°n)\n",
    "    topk_indices = topk_indices.cpu().numpy()\n",
    "    topk_values = topk_values.cpu().numpy()\n",
    "    \n",
    "    # 4. T·ªïng h·ª£p nh√£n t·ª´ h√†ng x√≥m (Weighted Voting)\n",
    "    for i, test_pid in enumerate(ids_test):\n",
    "        # Dict l∆∞u ƒëi·ªÉm s·ªë cho t·ª´ng nh√£n: {Term: Score}\n",
    "        term_scores = {}\n",
    "        \n",
    "        for k in range(TOP_K):\n",
    "            neighbor_idx = topk_indices[i, k]\n",
    "            score = topk_values[i, k] # ƒê·ªô gi·ªëng nhau (v√≠ d·ª• 0.95)\n",
    "            \n",
    "            neighbor_id = idx_to_trainid[neighbor_idx]\n",
    "            \n",
    "            # N·∫øu h√†ng x√≥m n√†y c√≥ nh√£n (c√≥ trong file train_terms)\n",
    "            if neighbor_id in train_labels_map:\n",
    "                neighbor_terms = train_labels_map[neighbor_id]\n",
    "                for term in neighbor_terms:\n",
    "                    # C·ªông d·ªìn ƒëi·ªÉm (Weighted Sum)\n",
    "                    if term not in term_scores:\n",
    "                        term_scores[term] = 0.0\n",
    "                    term_scores[term] += score\n",
    "        \n",
    "        # Chu·∫©n h√≥a ƒëi·ªÉm s·ªë (Chia cho t·ªïng tr·ªçng s·ªë ho·∫∑c K)\n",
    "        # ·ªû ƒë√¢y ta chia cho K ƒë·ªÉ score n·∫±m trong kho·∫£ng 0-1\n",
    "        # Ho·∫∑c ƒë∆°n gi·∫£n l√† gi·ªØ nguy√™n v√¨ CAFA ch·∫•m rank\n",
    "        \n",
    "        # L·∫•y Top 50 nh√£n ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ ghi file\n",
    "        sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "        \n",
    "        for term, total_score in sorted_terms:\n",
    "            # Normalize heuristic: Score trung b√¨nh\n",
    "            final_score = total_score / TOP_K \n",
    "            # Ch·ªâ ghi n·∫øu score ƒë·ªß l·ªõn\n",
    "            if final_score > 0.01: \n",
    "                output_lines.append(f\"{test_pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "    # D·ªçn d·∫πp b·ªô nh·ªõ GPU\n",
    "    del X_test, sim_matrix, topk_values, topk_indices\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d74bf97-e061-4483-8957-2a15ce2d462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u submission_knn_esm2.tsv...\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8eb4d-2a29-49ad-9fa1-a055d73e2fed",
   "metadata": {},
   "source": [
    "#### + Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d76c71-b46c-45a1-bcf2-fdb531185c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_knn_esm2.tsv\" \n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_knn_propagated.tsv\"\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1f094f-447a-4511-b7f7-f891878a82dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªçc file go-basic.obo...\n",
      "ƒêang x√¢y d·ª±ng quan h·ªá t·ªï ti√™n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 158229.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc /workspace/notebooks/submission_knn_esm2.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4549325it [00:01, 2344697.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:12<00:00, 18368.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_knn_propagated.tsv...\n",
      "XONG!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load c√¢y Gene Ontology\n",
    "print(\"ƒê·ªçc file go-basic.obo...\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "\n",
    "# 2. X√¢y d·ª±ng b·∫£n ƒë·ªì Cha-Con\n",
    "print(\"ƒêang x√¢y d·ª±ng quan h·ªá t·ªï ti√™n...\")\n",
    "ancestors_map = {}\n",
    "# Ch·ªâ quan t√¢m ƒë·∫øn c√°c node c√≥ quan h·ªá 'is_a' v√† 'part_of'\n",
    "for node in tqdm(graph.nodes()):\n",
    "    try:\n",
    "        # L·∫•y t·∫•t c·∫£ t·ªï ti√™n c·ªßa node hi·ªán t·∫°i\n",
    "        ancestors = networkx.descendants(graph, node)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 3. ƒê·ªçc file KNN\n",
    "print(f\"ƒêang ƒë·ªçc {INPUT_FILE}...\")\n",
    "preds = {} # {ProteinID: {Term: Score}}\n",
    "\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        if pid not in preds: preds[pid] = {}\n",
    "        preds[pid][term] = score\n",
    "\n",
    "# 4. Lan truy·ªÅn ƒëi·ªÉm s·ªë (Propagation)\n",
    "print(\"ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\")\n",
    "output_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # Copy dict ƒëi·ªÉm c≈©\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng term ƒëang c√≥\n",
    "    for term, score in term_scores.items():\n",
    "        # N·∫øu term n√†y c√≥ t·ªï ti√™n\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # Quy t·∫Øc c·ªët l√µi: ƒêi·ªÉm c·ªßa Cha lu√¥n >= ƒêi·ªÉm c·ªßa Con\n",
    "                current_p_score = new_scores.get(parent, 0.0)\n",
    "                new_scores[parent] = max(current_p_score, score)\n",
    "    \n",
    "    # L·ªçc v√† Ghi file\n",
    "    # Sau khi lan truy·ªÅn, s·ªë l∆∞·ª£ng nh√£n s·∫Ω ph√¨nh to ra (v√¨ th√™m cha √¥ng)\n",
    "    # Ta ch·ªâ l·∫•y Top 70 nh√£n ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ file kh√¥ng qu√° n·∫∑ng\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n",
    "    \n",
    "    for term, score in sorted_terms:\n",
    "        # L·ªçc b·ªõt r√°c: Ch·ªâ l·∫•y > 0.01\n",
    "        if score > 0.01:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "# 5. L∆∞u file\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"XONG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4214e-e234-4b39-8bd0-617d4aca1e30",
   "metadata": {},
   "source": [
    "#### Score: 0.211"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aba04a-d9ca-45b2-a0a5-5b7a55aacafd",
   "metadata": {},
   "source": [
    "### Taking Taxonomy into consideration\n",
    "M·ªôt protein c·ªßa Ng∆∞·ªùi (Human) v√† c·ªßa Vi khu·∫©n (E. coli) c√≥ th·ªÉ nh√¨n h∆°i gi·ªëng nhau (tr√¨nh t·ª± t∆∞∆°ng ƒë·ªìng), nh∆∞ng ch·ª©c nƒÉng th√¨ kh√°c m·ªôt tr·ªùi m·ªôt v·ª±c.\n",
    "\n",
    "N·∫øu KNN t√¨m th·∫•y \"h√†ng x√≥m\" l√† vi khu·∫©n ƒë·ªÉ g√°n nh√£n cho ng∆∞·ªùi, ta s·∫Ω sai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725ecab-d79e-448b-bb2d-1136d49e3342",
   "metadata": {},
   "source": [
    "### Taxonomy-aware KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748481d-8118-40b0-869e-f5736f265b26",
   "metadata": {},
   "source": [
    "#### Strategy: ∆∞u ti√™n c√πng lo√†i: Trong c√°c neighbours c·ªßa ƒëi·ªÉm c·∫ßn label, \n",
    "N·∫øu Test Protein l√† Ng∆∞·ªùi (9606) v√† h√†ng x√≥m t√¨m ƒë∆∞·ª£c c≈©ng l√† Ng∆∞·ªùi (9606) $\\rightarrow$ Nh√¢n ƒëi·ªÉm s·ªë l√™n 1.5 l·∫ßn (∆Øu ti√™n c·ª±c m·∫°nh).N·∫øu kh√°c lo√†i $\\rightarrow$ Gi·ªØ nguy√™n ho·∫∑c gi·∫£m nh·∫π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ada4d3-f478-4136-9472-499ee3b2dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\"\n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"    \n",
    "TRAIN_TAX_PATH   = \"/workspace/data/Train/train_taxonomy.tsv\"  \n",
    "TEST_TAX_PATH    = \"/workspace/data/Test/testsuperset-taxon-list.tsv\" \n",
    "\n",
    "TEST_CHUNKS_DIR  = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE      = \"/workspace/notebooks/submission_knn_tax_v2.tsv\"\n",
    "\n",
    "TOP_K = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11cc5aa-ce25-41dc-ab90-86314af76e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Taxonomy Map...\n",
      "ƒê√£ load 82403 train TaxIDs.\n",
      "ƒêang ƒë·ªçc /workspace/data/Test/testsuperset-taxon-list.tsv...\n",
      "ƒê√£ load 8453 test TaxIDs.\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Taxonomy Map...\")\n",
    "\n",
    "# 1. Load Train Taxonomy\n",
    "# File n√†y format: EntryID \\t TaxID\n",
    "try:\n",
    "    df_train_tax = pd.read_csv(TRAIN_TAX_PATH, sep=\"\\t\")\n",
    "    # C·∫ßn ƒë·∫£m b·∫£o t√™n c·ªôt ƒë√∫ng. Th∆∞·ªùng l√† 'EntryID' v√† 'TaxonomyID' (ho·∫∑c 'taxon_id')\n",
    "    # Ta convert sang dict cho nhanh: {PID: TaxID}\n",
    "    cols = df_train_tax.columns\n",
    "    train_tax_map = dict(zip(df_train_tax[cols[0]], df_train_tax[cols[1]]))\n",
    "    print(f\"ƒê√£ load {len(train_tax_map)} train TaxIDs.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback n·∫øu ƒë∆∞·ªùng d·∫´n sai\n",
    "    print(f\"Kh√¥ng t√¨m th·∫•y {TRAIN_TAX_PATH}, th·ª≠ t√¨m ·ªü th∆∞ m·ª•c g·ªëc...\")\n",
    "    TRAIN_TAX_PATH = \"/workspace/data/Train/train_taxonomy.tsv\"\n",
    "    df_train_tax = pd.read_csv(TRAIN_TAX_PATH, sep=\"\\t\")\n",
    "    cols = df_train_tax.columns\n",
    "    train_tax_map = dict(zip(df_train_tax[cols[0]], df_train_tax[cols[1]]))\n",
    "\n",
    "# 2. Load Test Taxonomy\n",
    "# File n√†y th∆∞·ªùng kh√¥ng c√≥ header ho·∫∑c header l·∫°, ta c·ª© ƒë·ªçc th·ª≠\n",
    "print(f\"ƒêang ƒë·ªçc {TEST_TAX_PATH}...\")\n",
    "try:\n",
    "    # Th·ª≠ ƒë·ªçc header=None tr∆∞·ªõc xem sao\n",
    "    df_test_tax = pd.read_csv(TEST_TAX_PATH, sep=\"\\t\", header=None, names=[\"EntryID\", \"TaxID\"])\n",
    "    # N·∫øu d√≤ng ƒë·∫ßu ti√™n l√† ch·ªØ (header), ta drop n√≥ ƒëi\n",
    "    if not str(df_test_tax.iloc[0, 1]).isdigit():\n",
    "        df_test_tax = pd.read_csv(TEST_TAX_PATH, sep=\"\\t\")\n",
    "        # ƒê·ªïi t√™n c·ªôt cho chu·∫©n\n",
    "        df_test_tax.columns = [\"EntryID\", \"TaxID\"]\n",
    "    \n",
    "    test_tax_map = dict(zip(df_test_tax[\"EntryID\"], df_test_tax[\"TaxID\"]))\n",
    "    print(f\"ƒê√£ load {len(test_tax_map)} test TaxIDs.\")\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói ƒë·ªçc file Test Tax: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8354dd-7d29-48c9-a9be-86aa3fbec92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Embeddings & Labels...\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Embeddings & Labels...\")\n",
    "# Load Train Embeddings\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "X_train = torch.from_numpy(X_train).to(device)\n",
    "norm = X_train.norm(p=2, dim=1, keepdim=True)\n",
    "X_train = X_train.div(norm)\n",
    "\n",
    "# Load Train IDs & Terms\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "train_labels_map = train_terms.groupby(\"EntryID\")[\"term\"].apply(set).to_dict()\n",
    "idx_to_trainid = {i: pid for i, pid in enumerate(train_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8330084a-f1ba-4d2b-b592-cb7249846296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu ch·∫°y Taxonomy-Aware KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:11<00:00,  3.99it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu ch·∫°y Taxonomy-Aware KNN...\")\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    # Load 1 c·ª•c Test\n",
    "    X_test_np = np.load(f_path)\n",
    "    X_test = torch.from_numpy(X_test_np).to(device)\n",
    "    norm_test = X_test.norm(p=2, dim=1, keepdim=True)\n",
    "    X_test = X_test.div(norm_test)\n",
    "    \n",
    "    # Load ID\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_test = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    # T√≠nh Cosine Sim\n",
    "    sim_matrix = torch.mm(X_test, X_train.t())\n",
    "    topk_values, topk_indices = torch.topk(sim_matrix, k=TOP_K, dim=1)\n",
    "    \n",
    "    topk_indices = topk_indices.cpu().numpy()\n",
    "    topk_values = topk_values.cpu().numpy()\n",
    "    \n",
    "    # X·ª≠ l√Ω Logic (Ph·∫ßn quan tr·ªçng nh·∫•t)\n",
    "    for i, test_pid in enumerate(ids_test):\n",
    "        # L·∫•y TaxID c·ªßa con test n√†y (M·∫∑c ƒë·ªãnh -1 n·∫øu kh√¥ng t√¨m th·∫•y)\n",
    "        test_taxon = test_tax_map.get(test_pid, -1)\n",
    "        \n",
    "        term_scores = {}\n",
    "        \n",
    "        for k in range(TOP_K):\n",
    "            neighbor_idx = topk_indices[i, k]\n",
    "            raw_score = topk_values[i, k]\n",
    "            \n",
    "            neighbor_pid = idx_to_trainid[neighbor_idx]\n",
    "            neighbor_taxon = train_tax_map.get(neighbor_pid, -2)\n",
    "            \n",
    "            # --- LOGIC TH∆Ø·ªûNG/PH·∫†T ---\n",
    "            weight = 1.0\n",
    "            \n",
    "            # N·∫øu c·∫£ 2 ƒë·ªÅu c√≥ th√¥ng tin lo√†i\n",
    "            if test_taxon != -1 and neighbor_taxon != -2:\n",
    "                if test_taxon == neighbor_taxon:\n",
    "                    # C√ôNG LO√ÄI: TƒÉng ƒë·ªô tin c·∫≠y l√™n 1.3 l·∫ßn\n",
    "                    weight = 1.3 \n",
    "                else:\n",
    "                    # KH√ÅC LO√ÄI: Gi·∫£m nh·∫π (v√¨ v·∫´n c√≥ th·ªÉ b·∫£o t·ªìn ch·ª©c nƒÉng)\n",
    "                    weight = 0.9\n",
    "            \n",
    "            final_score = raw_score * weight\n",
    "            \n",
    "            # C·ªông d·ªìn ƒëi·ªÉm cho c√°c ch·ª©c nƒÉng\n",
    "            if neighbor_pid in train_labels_map:\n",
    "                for term in train_labels_map[neighbor_pid]:\n",
    "                    if term not in term_scores: term_scores[term] = 0.0\n",
    "                    term_scores[term] += final_score\n",
    "        \n",
    "        # L·∫•y k·∫øt qu·∫£ (Top 60)\n",
    "        sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)[:60]\n",
    "        \n",
    "        for term, total_score in sorted_terms:\n",
    "            # Normalize ƒëi·ªÉm s·ªë (chia cho s·ªë h√†ng x√≥m ho·∫∑c tr·ªçng s·ªë)\n",
    "            # ·ªû ƒë√¢y chia cho TOP_K ƒë·ªÉ ƒë∆∞a v·ªÅ range 0-1 (t∆∞∆°ng ƒë·ªëi)\n",
    "            norm_score = total_score / TOP_K\n",
    "            \n",
    "            # N·∫øu c√≥ boost th√¨ norm_score c√≥ th·ªÉ > 1.0 -> Clip v·ªÅ 1.0\n",
    "            norm_score = min(norm_score, 1.0)\n",
    "            \n",
    "            if norm_score > 0.01:\n",
    "                output_lines.append(f\"{test_pid}\\t{term}\\t{norm_score:.3f}\")\n",
    "\n",
    "    del X_test, sim_matrix\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46420217-20de-4c00-a7ae-261d68eb262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_knn_tax_v2.tsv...\n",
      "XONG!\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"XONG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e971044-2a62-4b83-820c-61e01fd06e59",
   "metadata": {},
   "source": [
    "#### + Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445bda1f-1ea9-4de4-b6b5-d5e0289a57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "ƒêang ƒë·ªçc c√¢y Gene Ontology...\n",
      "ƒêang map quan h·ªá Con -> Cha √îng...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 153223.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc /workspace/notebooks/submission_knn_tax_v2.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7872336it [00:03, 2271903.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:13<00:00, 16100.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_knn_tax_propagated.tsv...\n",
      "------------------------------\n",
      "XONG!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q obonet networkx\n",
    "\n",
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_knn_tax_v2.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_knn_tax_propagated.tsv\"\n",
    "\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: X√ÇY D·ª∞NG B·∫¢N ƒê·ªí T·ªî TI√äN ---\n",
    "print(\"ƒêang ƒë·ªçc c√¢y Gene Ontology...\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "\n",
    "print(\"ƒêang map quan h·ªá Con -> Cha √îng...\")\n",
    "ancestors_map = {}\n",
    "# Ch·ªâ quan t√¢m c√°c node c√≥ trong graph\n",
    "for node in tqdm(graph.nodes()):\n",
    "    try:\n",
    "        # networkx.descendants trong obonet tr·∫£ v·ªÅ c√°c node \"cha/√¥ng\" (superclasses)\n",
    "        ancestors = networkx.descendants(graph, node)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- B∆Ø·ªöC 2: ƒê·ªåC FILE D·ª∞ ƒêO√ÅN ---\n",
    "print(f\"ƒêang ƒë·ªçc {INPUT_FILE}...\")\n",
    "preds = {} # {PID: {Term: Score}}\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "            \n",
    "            if pid not in preds: preds[pid] = {}\n",
    "            preds[pid][term] = score\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}. B·∫°n ƒë√£ ch·∫°y xong b∆∞·ªõc KNN Taxonomy ch∆∞a?\")\n",
    "    raise\n",
    "\n",
    "# --- B∆Ø·ªöC 3: LAN TRUY·ªÄN (PROPAGATION) ---\n",
    "print(\"ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\")\n",
    "output_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # Copy dict ƒëi·ªÉm c≈© sang dict m·ªõi ƒë·ªÉ c·∫≠p nh·∫≠t\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng term g·ªëc\n",
    "    for term, score in term_scores.items():\n",
    "        # N·∫øu term n√†y c√≥ t·ªï ti√™n trong c√¢y\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # QUY T·∫ÆC V√ÄNG: ƒêi·ªÉm Cha = Max(ƒêi·ªÉm Cha c≈©, ƒêi·ªÉm Con)\n",
    "                current_p_score = new_scores.get(parent, 0.0)\n",
    "                if score > current_p_score:\n",
    "                    new_scores[parent] = score\n",
    "    \n",
    "    # --- B∆Ø·ªöC 4: L·ªåC & GHI ---\n",
    "    # Sau khi lan truy·ªÅn, s·ªë l∆∞·ª£ng nh√£n s·∫Ω tƒÉng l√™n r·∫•t nhi·ªÅu.\n",
    "    # Ta c·∫ßn l·ªçc l·∫•y Top K ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ file kh√¥ng b·ªã qu√° n·∫∑ng (g√¢y l·ªói submit).\n",
    "    \n",
    "    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # L·∫•y Top 75 (ƒê·ªß ƒë·ªÉ bao ph·ªß c·∫£ cha l·∫´n con)\n",
    "    # L·∫•y nhi·ªÅu h∆°n 75 file s·∫Ω > 150MB, d·ªÖ b·ªã l·ªói timeout khi n·ªôp\n",
    "    for term, score in sorted_terms[:75]:\n",
    "        # Ch·ªâ l·∫•y ƒëi·ªÉm > 0.01 (b·ªè r√°c)\n",
    "        if score > 0.01:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "# L∆∞u file\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"XONG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe6b0f-83b1-4c0e-a56f-1c3dfb7b8a8c",
   "metadata": {},
   "source": [
    "#### Score: 0.234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25971e2e-4ac5-4b39-92f6-ef3e3641fb1b",
   "metadata": {},
   "source": [
    "#### Drawbacks of KNN: \"Hai protein g·∫ßn nhau th√¨ ch·ª©c nƒÉng g·∫ßn nhau\", th·ª±c t·∫ø c√≥ nh·ª©ng sequences ch·ªâ c√≥ 1-2 acid amin gi·ªëng nhau nh∆∞ng l·∫°i c√≥ ch·ª©c nƒÉng t∆∞∆°ng t·ª± and vice versa. Plus, KNN coi 1280 chi·ªÅu l√† nh∆∞ nhau, major vote c≈©ng ch∆∞a c√≥ tr·ªçng s·ªë."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48901fa-0f29-4609-adce-a9b882858f47",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a190ce-2aef-44eb-8902-77799d4c0a93",
   "metadata": {},
   "source": [
    "#### X√¢y d·ª±ng 1 m·∫°ng neuron ƒë·∫∑t l√™n c√°c vector embeddings. ƒê·∫ßu ra: 1500 nh√£n (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8017e81-5932-4a05-aec2-578ddfb8671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\"\n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "MODEL_SAVE_PATH = \"/workspace/models/mlp_model.pth\"\n",
    "BATCH_SIZE = 128 # TƒÉng batch size l√™n v√¨ embedding nh·∫π\n",
    "EPOCHS = 15      # Train k·ªπ m·ªôt ch√∫t\n",
    "LEARNING_RATE = 1e-3\n",
    "TOP_LABELS = 1500 \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdc1456-1371-45dd-9811-107c11f623c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load d·ªØ li·ªáu l√™n RAM...\n",
      "ƒêang x·ª≠ l√Ω nh√£n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76297it [00:00, 278218.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (82404, 1280), Y shape: (82404, 1500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ƒêang load d·ªØ li·ªáu l√™n RAM...\")\n",
    "\n",
    "# 1. Load Embeddings (X)\n",
    "X_all = np.load(TRAIN_EMB_PATH)\n",
    "ids_all = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "\n",
    "# 2. Load Labels (Y)\n",
    "print(\"ƒêang x·ª≠ l√Ω nh√£n...\")\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "# L·ªçc IDs\n",
    "ids_set = set(ids_all)\n",
    "train_terms = train_terms[train_terms[\"EntryID\"].isin(ids_set)]\n",
    "\n",
    "# L·∫•y Top 1500 terms\n",
    "top_term_list = train_terms[\"term\"].value_counts().head(TOP_LABELS).index.tolist()\n",
    "term_to_idx = {t: i for i, t in enumerate(top_term_list)}\n",
    "\n",
    "# T·∫°o Y matrix (Numpy)\n",
    "# C√°ch t·∫°o nhanh h∆°n pivot_table:\n",
    "Y_all = np.zeros((len(ids_all), TOP_LABELS), dtype=np.float32)\n",
    "id_to_idx = {pid: i for i, pid in enumerate(ids_all)}\n",
    "\n",
    "# Duy·ªát qua file terms ƒë·ªÉ ƒëi·ªÅn s·ªë 1 v√†o ma tr·∫≠n\n",
    "# (C√°ch n√†y ti·∫øt ki·ªám RAM h∆°n pivot table)\n",
    "grouped = train_terms[train_terms[\"term\"].isin(top_term_list)].groupby(\"EntryID\")[\"term\"].apply(list)\n",
    "\n",
    "for pid, terms in tqdm(grouped.items()):\n",
    "    if pid in id_to_idx:\n",
    "        row_idx = id_to_idx[pid]\n",
    "        col_idxs = [term_to_idx[t] for t in terms]\n",
    "        Y_all[row_idx, col_idxs] = 1.0\n",
    "\n",
    "print(f\"X shape: {X_all.shape}, Y shape: {Y_all.shape}\")\n",
    "\n",
    "# 3. Chia Train/Val\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_all, Y_all, test_size=0.1, random_state=42)\n",
    "\n",
    "# Chuy·ªÉn sang Tensor\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(Y_val).float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# D·ªçn RAM\n",
    "del X_all, Y_all, train_terms, grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8742c1b-6d92-4aac-bacc-668a748f0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X√ÇY D·ª∞NG MODEL (MLP)\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512), # Gi√∫p h·ªôi t·ª• nhanh\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),     # Ch·ªëng h·ªçc v·∫πt (Overfitting)\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Output Layer\n",
    "            nn.Linear(256, num_classes)\n",
    "            # Kh√¥ng d√πng Sigmoid ·ªü ƒë√¢y v√¨ BCEWithLogitsLoss ƒë√£ t√≠ch h·ª£p s·∫µn\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = ProteinClassifier(input_dim=1280, num_classes=TOP_LABELS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss() # Loss chu·∫©n cho Multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a414d65-6a04-4a6a-8998-c2cbe78ece68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu Train Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0319 | Val Loss = 0.0140\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0139 | Val Loss = 0.0130\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0131 | Val Loss = 0.0125\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0126 | Val Loss = 0.0121\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0123 | Val Loss = 0.0119\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0121 | Val Loss = 0.0117\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0119 | Val Loss = 0.0116\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0118 | Val Loss = 0.0115\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0117 | Val Loss = 0.0114\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0116 | Val Loss = 0.0114\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.0114 | Val Loss = 0.0113\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.0113 | Val Loss = 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.0113 | Val Loss = 0.0112\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.0112 | Val Loss = 0.0112\n",
      "  --> Saved Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.0111 | Val Loss = 0.0111\n",
      "  --> Saved Best Model\n",
      "Train xong!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu Train Neural Network...\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(\"  --> Saved Best Model\")\n",
    "\n",
    "print(\"Train xong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f84085-7302-4ab7-8cb1-c49987efc6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang d·ª± ƒëo√°n t·∫≠p Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:21<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang d·ª± ƒëo√°n t·∫≠p Test...\")\n",
    "import glob\n",
    "\n",
    "# Load l·∫°i best model\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.eval()\n",
    "\n",
    "chunk_files = sorted(glob.glob(\"/workspace/data/Embeddings/embeddings_chunks/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "# Danh s√°ch t√™n c√°c nh√£n\n",
    "terms_columns = top_term_list\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    X_chunk = np.load(f_path)\n",
    "    # Chuy·ªÉn sang Tensor\n",
    "    X_tensor = torch.from_numpy(X_chunk).float().to(device)\n",
    "    \n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_chunk = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy() # Chuy·ªÉn v·ªÅ x√°c su·∫•t 0-1\n",
    "        \n",
    "    # Ghi k·∫øt qu·∫£\n",
    "    for i, pid in enumerate(ids_chunk):\n",
    "        prob_row = probs[i]\n",
    "        # L·∫•y Top 50 cao nh·∫•t\n",
    "        top_indices = np.argsort(prob_row)[-50:]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            score = prob_row[idx]\n",
    "            term = terms_columns[idx]\n",
    "            if score > 0.01:\n",
    "                output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "                \n",
    "    del X_chunk, X_tensor\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefef696-3fdf-4363-a429-a2c92fb35ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ ƒêang l∆∞u /workspace/notebooks/submission_mlp_esm2.tsv...\n",
      "HO√ÄN T·∫§T\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = \"/workspace/notebooks/submission_mlp_esm2.tsv\"\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"HO√ÄN T·∫§T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba3f00-20a7-43d9-8680-5d39d475881b",
   "metadata": {},
   "source": [
    "#### + Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b6ab4c8-b9a2-4455-96cc-cb98f1307ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mƒêang ƒë·ªçc c·∫•u tr√∫c Gene Ontology...\n",
      "ƒêang map quan h·ªá (Con -> Cha √îng)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 160248.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc file /workspace/notebooks/submission_mlp_esm2.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7982025it [00:03, 2331406.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Logic: ƒêi·ªÉm Cha = Max(Cha c≈©, Con))...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:12<00:00, 18075.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_mlp_propagated.tsv...\n",
      "------------------------------\n",
      "HO√ÄN TH√ÄNH!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q obonet networkx\n",
    "\n",
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_mlp_esm2.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_mlp_propagated.tsv\"\n",
    "\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: X√ÇY D·ª∞NG B·∫¢N ƒê·ªí T·ªî TI√äN ---\n",
    "print(\"ƒêang ƒë·ªçc c·∫•u tr√∫c Gene Ontology...\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "\n",
    "print(\"ƒêang map quan h·ªá (Con -> Cha √îng)...\")\n",
    "ancestors_map = {}\n",
    "# Duy·ªát qua t·∫•t c·∫£ c√°c node ƒë·ªÉ t√¨m t·ªï ti√™n tr∆∞·ªõc (Pre-compute)\n",
    "for node in tqdm(graph.nodes()):\n",
    "    try:\n",
    "        # networkx.descendants tr·∫£ v·ªÅ t·∫≠p h·ª£p c√°c node cha/√¥ng\n",
    "        ancestors = networkx.descendants(graph, node)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- B∆Ø·ªöC 2: ƒê·ªåC FILE D·ª∞ ƒêO√ÅN ---\n",
    "print(f\"ƒêang ƒë·ªçc file {INPUT_FILE}...\")\n",
    "preds = {} # C·∫•u tr√∫c: {ProteinID: {Term: Score}}\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "            \n",
    "            if pid not in preds: preds[pid] = {}\n",
    "            preds[pid][term] = score\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}. B·∫°n ƒë√£ ch·∫°y xong b∆∞·ªõc Train MLP ch∆∞a?\")\n",
    "    raise\n",
    "\n",
    "# --- B∆Ø·ªöC 3: LAN TRUY·ªÄN (PROPAGATION) ---\n",
    "print(\"ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Logic: ƒêi·ªÉm Cha = Max(Cha c≈©, Con))...\")\n",
    "output_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # T·∫°o b·∫£n sao ƒë·ªÉ c·∫≠p nh·∫≠t ƒëi·ªÉm\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng term con ƒëang c√≥ ƒëi·ªÉm\n",
    "    for term, score in term_scores.items():\n",
    "        # N·∫øu term n√†y c√≥ cha √¥ng trong b·∫£n ƒë·ªì\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # C·∫≠p nh·∫≠t ƒëi·ªÉm cho cha\n",
    "                current_p_score = new_scores.get(parent, 0.0)\n",
    "                # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu ƒëi·ªÉm m·ªõi cao h∆°n ƒëi·ªÉm c≈©\n",
    "                if score > current_p_score:\n",
    "                    new_scores[parent] = score\n",
    "    \n",
    "    # --- B∆Ø·ªöC 4: L·ªåC TOP K ƒê·ªÇ FILE KH√îNG B·ªä N·∫∂NG ---\n",
    "    # Sau khi lan truy·ªÅn, m·ªôt protein c√≥ th·ªÉ c√≥ h√†ng trƒÉm nh√£n cha.\n",
    "    # Ta c·∫ßn l·ªçc l·∫•y Top 75 ƒëi·ªÉm cao nh·∫•t.\n",
    "    \n",
    "    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # L·∫•y Top 75\n",
    "    for term, score in sorted_terms[:75]:\n",
    "        # Ch·ªâ l·∫•y ƒëi·ªÉm > 0.01 ƒë·ªÉ b·ªè r√°c\n",
    "        if score > 0.01:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 5: L∆ØU FILE ---\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"HO√ÄN TH√ÄNH!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3536b-9aba-48d3-ada6-ad385bc7340f",
   "metadata": {},
   "source": [
    "#### Score: 0.204 -> H·ªçc v·∫πt ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9a44c-d51d-4766-b1c7-8f6f00fd10f6",
   "metadata": {},
   "source": [
    "### Ensemble: KNN + BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edee84fc-7514-4e91-8c9e-48573aef7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ c√≥ s·∫µn file BLAST.\n",
      "ƒêang load d·ªØ li·ªáu...\n",
      "ƒêang x·ª≠ l√Ω BLAST...\n",
      "ƒêang ƒë·ªçc file KNN: /workspace/notebooks/submission_knn_tax_propagated.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16226847it [00:10, 1523809.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang tr·ªôn BLAST + KNN (L·∫•y ƒëi·ªÉm cao nh·∫•t)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16710313/16710313 [00:19<00:00, 849071.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_hybrid_best_v1.tsv...\n",
      "XONG! H√£y n·ªôp file n√†y.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# 1. File KNN t·ªët nh·∫•t c·ªßa b·∫°n (File ƒë·∫°t 0.234)\n",
    "# L∆∞u √Ω: Ch·ªçn ƒë√∫ng file output c·ªßa b∆∞·ªõc KNN c√≥ Taxonomy + Propagation\n",
    "KNN_FILE = \"/workspace/notebooks/submission_knn_tax_propagated.tsv\" \n",
    "\n",
    "# 2. File k·∫øt qu·∫£ BLAST (Level 2)\n",
    "# N·∫øu b·∫°n ch∆∞a ch·∫°y BLAST ho·∫∑c ƒë√£ x√≥a, ƒëo·∫°n code d∆∞·ªõi s·∫Ω t·ª± ch·∫°y l·∫°i (m·∫•t 3 ph√∫t)\n",
    "BLAST_RESULT = \"/workspace/data/diamond_results.tsv\"\n",
    "TRAIN_FASTA = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "TEST_FASTA = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "DB_PATH = \"/workspace/data/Train/train_data.dmnd\"\n",
    "DIAMOND_BIN = \"/usr/bin/diamond\" # Ho·∫∑c \"diamond\" n·∫øu ƒë√£ add path\n",
    "\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_hybrid_best_v1.tsv\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: KI·ªÇM TRA V√Ä CH·∫†Y BLAST N·∫æU C·∫¶N ---\n",
    "if not os.path.exists(BLAST_RESULT):\n",
    "    print(\"Kh√¥ng th·∫•y file BLAST c≈©, ƒëang ch·∫°y l·∫°i...\")\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.system(f\"{DIAMOND_BIN} makedb --in {TRAIN_FASTA} -d {DB_PATH} --quiet\")\n",
    "    \n",
    "    # Ch·∫°y so kh·ªõp\n",
    "    cmd = f\"{DIAMOND_BIN} blastp -d {DB_PATH} -q {TEST_FASTA} -o {BLAST_RESULT} --sensitive --top 1 -f 6 qseqid sseqid pident\"\n",
    "    os.system(cmd)\n",
    "    print(\"BLAST xong!\")\n",
    "else:\n",
    "    print(\"ƒê√£ c√≥ s·∫µn file BLAST.\")\n",
    "\n",
    "# --- B∆Ø·ªöC 2: ƒê·ªåC D·ªÆ LI·ªÜU ---\n",
    "print(\"ƒêang load d·ªØ li·ªáu...\")\n",
    "\n",
    "# 1. Load BLAST Predictions\n",
    "# Logic: Ch·ªâ tin t∆∞·ªüng BLAST n·∫øu ƒë·ªô gi·ªëng nhau > 30%\n",
    "# Map TrainID sang Terms\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "train_terms_grouped = train_terms.groupby(\"EntryID\")[\"term\"].apply(list).to_dict()\n",
    "\n",
    "blast_preds = {}\n",
    "df_blast = pd.read_csv(BLAST_RESULT, sep=\"\\t\", names=[\"test_id\", \"train_id\", \"pident\"])\n",
    "\n",
    "print(\"ƒêang x·ª≠ l√Ω BLAST...\")\n",
    "for _, row in df_blast.iterrows():\n",
    "    pident = row['pident'] / 100.0\n",
    "    if pident < 0.3: continue # B·ªè qua n·∫øu gi·ªëng nhau qu√° √≠t\n",
    "    \n",
    "    test_id = str(row['test_id']).split(\"|\")[1] if \"|\" in str(row['test_id']) else str(row['test_id'])\n",
    "    train_id = str(row['train_id']).split(\"|\")[1] if \"|\" in str(row['train_id']) else str(row['train_id'])\n",
    "    \n",
    "    if train_id in train_terms_grouped:\n",
    "        for term in train_terms_grouped[train_id]:\n",
    "            # BLAST score r·∫•t uy t√≠n\n",
    "            blast_preds[(test_id, term)] = pident\n",
    "\n",
    "# 2. Load KNN Predictions (File 0.234)\n",
    "print(f\"ƒêang ƒë·ªçc file KNN: {KNN_FILE}...\")\n",
    "knn_preds = {}\n",
    "try:\n",
    "    with open(KNN_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "            knn_preds[(pid, term)] = score\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file {KNN_FILE}. B·∫°n h√£y ch·∫Øc ch·∫Øn ƒë√£ ch·∫°y xong b∆∞·ªõc KNN Tax Propagation.\")\n",
    "    raise\n",
    "\n",
    "# --- B∆Ø·ªöC 3: TR·ªòN (MAX STRATEGY) ---\n",
    "print(\"ƒêang tr·ªôn BLAST + KNN (L·∫•y ƒëi·ªÉm cao nh·∫•t)...\")\n",
    "# Logic: Final_Score = Max(BLAST, KNN)\n",
    "# N·∫øu BLAST t√¨m th·∫•y (ƒë·ªô tin c·∫≠y cao), n√≥ s·∫Ω ghi ƒë√® l√™n KNN.\n",
    "# N·∫øu BLAST b√≥ tay, ta d√πng KNN.\n",
    "\n",
    "all_keys = set(blast_preds.keys()) | set(knn_preds.keys())\n",
    "output_lines = []\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    s_blast = blast_preds.get(key, 0.0)\n",
    "    s_knn = knn_preds.get(key, 0.0)\n",
    "    \n",
    "    final_score = max(s_blast, s_knn)\n",
    "    \n",
    "    # L·ªçc r√°c l·∫ßn cu·ªëi\n",
    "    if final_score > 0.01:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 4: L∆ØU FILE ---\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"XONG! H√£y n·ªôp file n√†y.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9d5be-4666-4de6-b06c-f6f0a918abf3",
   "metadata": {},
   "source": [
    "#### Score: 0.225"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49beaf9-ba0e-44f9-8696-41a90f37ea37",
   "metadata": {},
   "source": [
    "### MLP with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a218b3ab-5801-4cfb-8f90-26fda20fb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load d·ªØ li·ªáu...\n",
      "B·∫Øt ƒë·∫ßu Train (C√≥ Early Stopping)...\n",
      "Epoch 1: Train=0.0540 | Val=0.0158 --> Saved Best Model\n",
      "Epoch 2: Train=0.0149 | Val=0.0142 --> Saved Best Model\n",
      "Epoch 3: Train=0.0141 | Val=0.0136 --> Saved Best Model\n",
      "Epoch 4: Train=0.0136 | Val=0.0132 --> Saved Best Model\n",
      "Epoch 5: Train=0.0132 | Val=0.0129 --> Saved Best Model\n",
      "Epoch 6: Train=0.0129 | Val=0.0126 --> Saved Best Model\n",
      "Epoch 7: Train=0.0126 | Val=0.0124 --> Saved Best Model\n",
      "Epoch 8: Train=0.0124 | Val=0.0123 --> Saved Best Model\n",
      "Epoch 9: Train=0.0122 | Val=0.0121 --> Saved Best Model\n",
      "Epoch 10: Train=0.0121 | Val=0.0119 --> Saved Best Model\n",
      "Epoch 11: Train=0.0119 | Val=0.0119 | Patience 1/5\n",
      "Epoch 12: Train=0.0118 | Val=0.0118 --> Saved Best Model\n",
      "Epoch 13: Train=0.0117 | Val=0.0118 | Patience 1/5\n",
      "Epoch 14: Train=0.0116 | Val=0.0116 --> Saved Best Model\n",
      "Epoch 15: Train=0.0115 | Val=0.0115 --> Saved Best Model\n",
      "Epoch 16: Train=0.0114 | Val=0.0115 --> Saved Best Model\n",
      "Epoch 17: Train=0.0114 | Val=0.0115 | Patience 1/5\n",
      "Epoch 18: Train=0.0113 | Val=0.0115 --> Saved Best Model\n",
      "Epoch 19: Train=0.0112 | Val=0.0114 --> Saved Best Model\n",
      "Epoch 20: Train=0.0112 | Val=0.0114 --> Saved Best Model\n",
      "Epoch 21: Train=0.0111 | Val=0.0114 | Patience 1/5\n",
      "Epoch 22: Train=0.0110 | Val=0.0114 --> Saved Best Model\n",
      "Epoch 23: Train=0.0110 | Val=0.0113 --> Saved Best Model\n",
      "Epoch 24: Train=0.0109 | Val=0.0112 --> Saved Best Model\n",
      "Epoch 25: Train=0.0109 | Val=0.0112 --> Saved Best Model\n",
      "Epoch 26: Train=0.0108 | Val=0.0113 | Patience 1/5\n",
      "Epoch 27: Train=0.0108 | Val=0.0112 | Patience 2/5\n",
      "Epoch 28: Train=0.0107 | Val=0.0111 --> Saved Best Model\n",
      "Epoch 29: Train=0.0107 | Val=0.0112 | Patience 1/5\n",
      "Epoch 30: Train=0.0106 | Val=0.0111 | Patience 2/5\n",
      "Epoch 31: Train=0.0106 | Val=0.0111 --> Saved Best Model\n",
      "Epoch 32: Train=0.0105 | Val=0.0111 | Patience 1/5\n",
      "Epoch 33: Train=0.0105 | Val=0.0112 | Patience 2/5\n",
      "Epoch 34: Train=0.0104 | Val=0.0111 | Patience 3/5\n",
      "Epoch 35: Train=0.0104 | Val=0.0111 | Patience 4/5\n",
      "Epoch 36: Train=0.0104 | Val=0.0111 --> Saved Best Model\n",
      "Epoch 37: Train=0.0103 | Val=0.0111 --> Saved Best Model\n",
      "Epoch 38: Train=0.0103 | Val=0.0111 | Patience 1/5\n",
      "Epoch 39: Train=0.0102 | Val=0.0111 | Patience 2/5\n",
      "Epoch 40: Train=0.0102 | Val=0.0111 --> Saved Best Model\n",
      "Epoch 41: Train=0.0102 | Val=0.0111 | Patience 1/5\n",
      "Epoch 42: Train=0.0101 | Val=0.0111 | Patience 2/5\n",
      "Epoch 43: Train=0.0101 | Val=0.0111 | Patience 3/5\n",
      "Epoch 44: Train=0.0101 | Val=0.0111 | Patience 4/5\n",
      "Epoch 45: Train=0.0100 | Val=0.0111 | Patience 5/5\n",
      "D·ª™NG S·ªöM! Model b·∫Øt ƒë·∫ßu h·ªçc v·∫πt t·∫°i Epoch 45\n",
      "ƒêang d·ª± ƒëo√°n t·∫≠p Test v·ªõi Best Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:23<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! H√£y ch·∫°y ti·∫øp Propagation cho file 'submission_mlp_tuned.tsv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\"\n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "MODEL_SAVE_PATH = \"/workspace/models/best_mlp_model.pth\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_mlp_tuned.tsv\"\n",
    "\n",
    "BATCH_SIZE = 256 # TƒÉng batch size ƒë·ªÉ gradient ·ªïn ƒë·ªãnh h∆°n\n",
    "EPOCHS = 50      # Cho max 50, nh∆∞ng Early Stop s·∫Ω d·ª´ng s·ªõm\n",
    "LEARNING_RATE = 5e-4 # Gi·∫£m LR xu·ªëng ƒë·ªÉ h·ªçc ch·∫≠m m√† ch·∫Øc (tr√°nh v·ªçt x√†)\n",
    "TOP_LABELS = 1500 \n",
    "PATIENCE = 5     # Cho ph√©p kh√¥ng ti·∫øn b·ªô 5 l·∫ßn, qu√° 5 l·∫ßn l√† c·∫Øt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU (Gi·ªëng h·ªát c≈©) ---\n",
    "print(\"ƒêang load d·ªØ li·ªáu...\")\n",
    "X_all = np.load(TRAIN_EMB_PATH)\n",
    "ids_all = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "ids_set = set(ids_all)\n",
    "train_terms = train_terms[train_terms[\"EntryID\"].isin(ids_set)]\n",
    "top_term_list = train_terms[\"term\"].value_counts().head(TOP_LABELS).index.tolist()\n",
    "term_to_idx = {t: i for i, t in enumerate(top_term_list)}\n",
    "\n",
    "Y_all = np.zeros((len(ids_all), TOP_LABELS), dtype=np.float32)\n",
    "id_to_idx = {pid: i for i, pid in enumerate(ids_all)}\n",
    "grouped = train_terms[train_terms[\"term\"].isin(top_term_list)].groupby(\"EntryID\")[\"term\"].apply(list)\n",
    "for pid, terms in grouped.items():\n",
    "    if pid in id_to_idx:\n",
    "        Y_all[id_to_idx[pid], [term_to_idx[t] for t in terms]] = 1.0\n",
    "\n",
    "# Chia t·∫≠p (Quan tr·ªçng: Stratify ƒë·ªÉ chia ƒë·ªÅu nh√£n kh√≥/d·ªÖ n·∫øu c√≥ th·ªÉ, nh∆∞ng multilabel kh√≥ stratify chu·∫©n n√™n random)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_all, Y_all, test_size=0.15, random_state=42) # TƒÉng Val l√™n 15%\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(Y_val).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "del X_all, Y_all, train_terms\n",
    "gc.collect()\n",
    "\n",
    "# --- B∆Ø·ªöC 2: MODEL C·∫¢I TI·∫æN ---\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024), # TƒÉng layer ƒë·∫ßu\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),     # Dropout m·∫°nh tay (50%) ƒë·ªÉ ph·∫°t h·ªçc v·∫πt\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = ProteinClassifier(1280, TOP_LABELS).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # AdamW ch·ªëng overfitting t·ªët h∆°n\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# --- B∆Ø·ªöC 3: TRAIN V·ªöI EARLY STOPPING ---\n",
    "print(\"B·∫Øt ƒë·∫ßu Train (C√≥ Early Stopping)...\")\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for bx, by in train_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in val_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            val_loss += criterion(model(bx), by).item()\n",
    "            \n",
    "    avg_train = train_loss / len(train_loader)\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train={avg_train:.4f} | Val={avg_val:.4f}\", end=\"\")\n",
    "    \n",
    "    # LOGIC EARLY STOPPING\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(\" --> Saved Best Model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" | Patience {patience_counter}/{PATIENCE}\")\n",
    "        \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"D·ª™NG S·ªöM! Model b·∫Øt ƒë·∫ßu h·ªçc v·∫πt t·∫°i Epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# --- B∆Ø·ªöC 4: D·ª∞ ƒêO√ÅN & GHI FILE ---\n",
    "# (Ph·∫ßn n√†y gi·ªØ nguy√™n logic c≈©, ch·ªâ load l·∫°i best model)\n",
    "print(\"ƒêang d·ª± ƒëo√°n t·∫≠p Test v·ªõi Best Model...\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.eval()\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "chunk_files = sorted(glob.glob(\"/workspace/data/Embeddings/embeddings_chunks/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "for f_path in tqdm(chunk_files):\n",
    "    X_chunk = np.load(f_path)\n",
    "    X_tensor = torch.from_numpy(X_chunk).float().to(device)\n",
    "    ids_chunk = np.load(f_path.replace(\"test_part_\", \"test_ids_\"), allow_pickle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(model(X_tensor)).cpu().numpy()\n",
    "        \n",
    "    for i, pid in enumerate(ids_chunk):\n",
    "        # L·∫•y Top 50, ng∆∞·ª°ng th·∫•p h∆°n 1 ch√∫t\n",
    "        top_idxs = np.argsort(probs[i])[-50:]\n",
    "        for idx in top_idxs:\n",
    "            s = probs[i][idx]\n",
    "            if s > 0.005: # H·∫° threshold xu·ªëng ch√∫t\n",
    "                output_lines.append(f\"{pid}\\t{top_term_list[idx]}\\t{s:.3f}\")\n",
    "                \n",
    "    del X_chunk, X_tensor\n",
    "    gc.collect()\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "print(\"Done! H√£y ch·∫°y ti·∫øp Propagation cho file 'submission_mlp_tuned.tsv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c85217-4ec5-4ba4-903f-25fcca1c47b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "ƒêang d√πng file OBO t·∫°i: /workspace/data/Train/go-basic.obo\n",
      "ƒêang ƒë·ªçc c·∫•u tr√∫c Gene Ontology...\n",
      "ƒêang map quan h·ªá (Con -> Cha √îng)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 154454.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc file /workspace/notebooks/submission_mlp_tuned.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9935666it [00:04, 2323627.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Logic: ƒêi·ªÉm Cha = Max(Cha c≈©, Con))...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:14<00:00, 14988.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_mlp_tuned_propagated.tsv...\n",
      "------------------------------\n",
      "HO√ÄN TH√ÄNH! File '/workspace/notebooks/submission_mlp_tuned_propagated.tsv' ƒë√£ s·∫µn s√†ng.\n",
      "H√£y n·ªôp file n√†y l√™n Kaggle v√† xem s·ª± kh√°c bi·ªát!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q obonet networkx\n",
    "\n",
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# 1. File ƒë·∫ßu v√†o: K·∫øt qu·∫£ c·ªßa MLP Tuned v·ª´a ch·∫°y\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_mlp_tuned.tsv\"\n",
    "\n",
    "# 2. File ƒë·∫ßu ra: File n·ªôp cu·ªëi c√πng\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_mlp_tuned_propagated.tsv\"\n",
    "\n",
    "# 3. T√¨m file OBO (C√¢y ph·∫£ h·ªá)\n",
    "if os.path.exists(\"/workspace/data/Train/go-basic.obo\"):\n",
    "    OBO_PATH = \"/workspace/data/Train/go-basic.obo\"\n",
    "else:\n",
    "    OBO_PATH = \"/workspace/data/go-basic.obo\"\n",
    "\n",
    "print(f\"ƒêang d√πng file OBO t·∫°i: {OBO_PATH}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 1: X√ÇY D·ª∞NG B·∫¢N ƒê·ªí T·ªî TI√äN ---\n",
    "print(\"ƒêang ƒë·ªçc c·∫•u tr√∫c Gene Ontology...\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "\n",
    "print(\"ƒêang map quan h·ªá (Con -> Cha √îng)...\")\n",
    "ancestors_map = {}\n",
    "# Duy·ªát qua t·∫•t c·∫£ c√°c node ƒë·ªÉ t√¨m t·ªï ti√™n (Pre-compute cho nhanh)\n",
    "for node in tqdm(graph.nodes()):\n",
    "    try:\n",
    "        # networkx.descendants tr·∫£ v·ªÅ t·∫≠p h·ª£p c√°c node cha/√¥ng\n",
    "        ancestors = networkx.descendants(graph, node)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- B∆Ø·ªöC 2: ƒê·ªåC FILE D·ª∞ ƒêO√ÅN ---\n",
    "print(f\"ƒêang ƒë·ªçc file {INPUT_FILE}...\")\n",
    "preds = {} # C·∫•u tr√∫c: {ProteinID: {Term: Score}}\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "            \n",
    "            if pid not in preds: preds[pid] = {}\n",
    "            preds[pid][term] = score\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}. B·∫°n ƒë√£ ch·∫°y xong b∆∞·ªõc Train MLP Tuned ch∆∞a?\")\n",
    "    raise\n",
    "\n",
    "# --- B∆Ø·ªöC 3: LAN TRUY·ªÄN (PROPAGATION) ---\n",
    "print(\"ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Logic: ƒêi·ªÉm Cha = Max(Cha c≈©, Con))...\")\n",
    "output_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # T·∫°o b·∫£n sao ƒë·ªÉ c·∫≠p nh·∫≠t ƒëi·ªÉm\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng term con ƒëang c√≥ ƒëi·ªÉm\n",
    "    for term, score in term_scores.items():\n",
    "        # N·∫øu term n√†y c√≥ cha √¥ng trong b·∫£n ƒë·ªì\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # C·∫≠p nh·∫≠t ƒëi·ªÉm cho cha\n",
    "                current_p_score = new_scores.get(parent, 0.0)\n",
    "                # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu ƒëi·ªÉm m·ªõi cao h∆°n ƒëi·ªÉm c≈©\n",
    "                if score > current_p_score:\n",
    "                    new_scores[parent] = score\n",
    "    \n",
    "    # --- B∆Ø·ªöC 4: L·ªåC & GHI FILE ---\n",
    "    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # L·∫•y Top 75 ƒëi·ªÉm cao nh·∫•t (ƒë·ªÉ file kh√¥ng qu√° n·∫∑ng)\n",
    "    for term, score in sorted_terms[:75]:\n",
    "        # Ch·ªâ l·∫•y ƒëi·ªÉm > 0.005\n",
    "        if score > 0.005:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 5: L∆ØU FILE ---\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"HO√ÄN TH√ÄNH! File '{OUTPUT_FILE}' ƒë√£ s·∫µn s√†ng.\")\n",
    "print(\"H√£y n·ªôp file n√†y l√™n Kaggle v√† xem s·ª± kh√°c bi·ªát!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb748b-8434-4f7e-a0b3-6da3d45ecb8e",
   "metadata": {},
   "source": [
    "#### Score: 0.217"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12828dc-3ff8-418f-8959-5823c349bf06",
   "metadata": {},
   "source": [
    "### Taxonomy-aware KNN + BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17265708-0771-4f2a-ab06-073f62cd167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ c√≥ s·∫µn k·∫øt qu·∫£ BLAST.\n",
      "ƒêang x·ª≠ l√Ω BLAST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262530/262530 [00:03<00:00, 74232.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc file KNN t·ªët nh·∫•t (/workspace/notebooks/submission_knn_tax_propagated.tsv)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16226847it [00:09, 1634421.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang tr·ªôn (Max Strategy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16693341/16693341 [00:18<00:00, 904218.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_FINAL_ENSEMBLE.tsv...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# 1. File KNN x·ªãn nh·∫•t (File ƒë·∫°t 0.234)\n",
    "# (ƒê·∫£m b·∫£o b·∫°n d√πng ƒë√∫ng file ƒë√£ ch·∫°y Propagation)\n",
    "KNN_FILE = \"/workspace/notebooks/submission_knn_tax_propagated.tsv\"\n",
    "\n",
    "# 2. C·∫•u h√¨nh ƒë·ªÉ ch·∫°y l·∫°i BLAST (n·∫øu c·∫ßn)\n",
    "BLAST_RESULT = \"/workspace/data/diamond_results.tsv\"\n",
    "TRAIN_FASTA = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "TEST_FASTA = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "DB_PATH = \"/workspace/data/Train/train_data.dmnd\"\n",
    "DIAMOND_BIN = \"/usr/bin/diamond\" # Ho·∫∑c ƒë∆∞·ªùng d·∫´n t·ªõi file diamond\n",
    "\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_FINAL_ENSEMBLE.tsv\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: ƒê·∫¢M B·∫¢O C√ì K·∫æT QU·∫¢ BLAST ---\n",
    "if not os.path.exists(BLAST_RESULT):\n",
    "    print(\"ƒêang ch·∫°y l·∫°i BLAST l·∫ßn cu·ªëi (cho ch·∫Øc ƒÉn)...\")\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.system(f\"{DIAMOND_BIN} makedb --in {TRAIN_FASTA} -d {DB_PATH} --quiet\")\n",
    "    \n",
    "    # Ch·∫°y Diamond (Sensitive mode)\n",
    "    cmd = f\"{DIAMOND_BIN} blastp -d {DB_PATH} -q {TEST_FASTA} -o {BLAST_RESULT} --sensitive --top 1 -f 6 qseqid sseqid pident\"\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    print(\"ƒê√£ c√≥ s·∫µn k·∫øt qu·∫£ BLAST.\")\n",
    "\n",
    "# --- B∆Ø·ªöC 2: LOAD D·ªÆ LI·ªÜU BLAST ---\n",
    "print(\"ƒêang x·ª≠ l√Ω BLAST...\")\n",
    "# Load mapping ID -> Terms\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "train_terms_grouped = train_terms.groupby(\"EntryID\")[\"term\"].apply(list).to_dict()\n",
    "\n",
    "blast_preds = {}\n",
    "df_blast = pd.read_csv(BLAST_RESULT, sep=\"\\t\", names=[\"test_id\", \"train_id\", \"pident\"])\n",
    "\n",
    "for _, row in tqdm(df_blast.iterrows(), total=len(df_blast)):\n",
    "    pident = row['pident'] / 100.0\n",
    "    # Ch·ªâ tin t∆∞·ªüng BLAST n·∫øu ƒë·ªô gi·ªëng > 35%\n",
    "    if pident < 0.35: continue\n",
    "    \n",
    "    # Clean ID\n",
    "    tid = str(row['test_id']).split(\"|\")[1] if \"|\" in str(row['test_id']) else str(row['test_id'])\n",
    "    trid = str(row['train_id']).split(\"|\")[1] if \"|\" in str(row['train_id']) else str(row['train_id'])\n",
    "    \n",
    "    if trid in train_terms_grouped:\n",
    "        for term in train_terms_grouped[trid]:\n",
    "            # L∆∞u ƒëi·ªÉm BLAST\n",
    "            blast_preds[(tid, term)] = pident\n",
    "\n",
    "# --- B∆Ø·ªöC 3: LOAD D·ªÆ LI·ªÜU KNN (0.234) ---\n",
    "print(f\"ƒêang ƒë·ªçc file KNN t·ªët nh·∫•t ({KNN_FILE})...\")\n",
    "knn_preds = {}\n",
    "try:\n",
    "    with open(KNN_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            knn_preds[(parts[0], parts[1])] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {KNN_FILE}. B·∫°n check l·∫°i t√™n file KNN 0.234 nh√©.\")\n",
    "    raise\n",
    "\n",
    "# --- B∆Ø·ªöC 4: H·ª¢P TH·ªÇ (MAX STRATEGY) ---\n",
    "print(\"ƒêang tr·ªôn (Max Strategy)...\")\n",
    "all_keys = set(blast_preds.keys()) | set(knn_preds.keys())\n",
    "output_lines = []\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    s_blast = blast_preds.get(key, 0.0)\n",
    "    s_knn = knn_preds.get(key, 0.0)\n",
    "    \n",
    "    # L·∫•y ƒëi·ªÉm cao nh·∫•t gi·ªØa 2 thu·∫≠t to√°n\n",
    "    final_score = max(s_blast, s_knn)\n",
    "    \n",
    "    if final_score > 0.01:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 5: L∆ØU FILE ---\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefd014-360b-489a-a3c0-834567a51a38",
   "metadata": {},
   "source": [
    "#### Score: 0.224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94727a98-6d66-4cfc-bccd-df6af2a3cd0d",
   "metadata": {},
   "source": [
    "### Weighted F1: Nh√£n hi·∫øm -> ƒëi·ªÉm cao. Nh∆∞ng KNN ƒëang x·ª≠ l√≠ c√°c nh√£n l√† nh∆∞ nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4451b-8144-4bad-a3b2-2cbd4ee14ecb",
   "metadata": {},
   "source": [
    "### Sol: IC weighting (Information Content weighting): Nh√£n n√†o hi·∫øm -> b∆°m ƒëi·ªÉm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ecf2f3-db94-4bec-957d-8fc8f80e300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t√≠nh to√°n ƒë·ªô hi·∫øm c·ªßa t·ª´ng nh√£n (IC)...\n",
      "‚úÖ ƒê√£ t√≠nh tr·ªçng s·ªë cho 26125 nh√£n.\n",
      "ƒêang ƒë·ªçc v√† boost ƒëi·ªÉm cho submission_knn_tax_propagated.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16226847it [00:11, 1391612.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u submission_knn_tax_IC_weighted.tsv...\n",
      "XONG!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# D√πng l·∫°i file t·ªët nh·∫•t c·ªßa b·∫°n (File ƒë·∫°t 0.234)\n",
    "# (ƒê·∫£m b·∫£o file n√†y ƒëang n·∫±m trong th∆∞ m·ª•c hi·ªán t·∫°i)\n",
    "INPUT_FILE = \"submission_knn_tax_propagated.tsv\"\n",
    "OUTPUT_FILE = \"submission_knn_tax_IC_weighted.tsv\"\n",
    "TRAIN_TERMS = \"/workspace/data/Train/train_terms.tsv\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: T√çNH ƒê·ªò HI·∫æM (INFORMATION CONTENT) ---\n",
    "print(\"ƒêang t√≠nh to√°n ƒë·ªô hi·∫øm c·ªßa t·ª´ng nh√£n (IC)...\")\n",
    "df_terms = pd.read_csv(TRAIN_TERMS, sep=\"\\t\", usecols=['term'])\n",
    "term_counts = df_terms['term'].value_counts().to_dict()\n",
    "total_proteins = len(df_terms)\n",
    "\n",
    "# H√†m t√≠nh IC: C√†ng hi·∫øm c√†ng cao\n",
    "# IC = -log2(Count / Total)\n",
    "term_weights = {}\n",
    "for term, count in term_counts.items():\n",
    "    prob = count / total_proteins\n",
    "    ic = -math.log2(prob)\n",
    "    # Chu·∫©n h√≥a nh·∫π ƒë·ªÉ kh√¥ng b∆°m qu√° l·ªë (Scale v·ªÅ kho·∫£ng 1.0 - 2.0)\n",
    "    # Nh√£n c·ª±c hi·∫øm (IC cao) s·∫Ω ƒë∆∞·ª£c nh√¢n h·ªá s·ªë l·ªõn\n",
    "    term_weights[term] = ic\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t√≠nh tr·ªçng s·ªë cho {len(term_weights)} nh√£n.\")\n",
    "\n",
    "# --- B∆Ø·ªöC 2: B∆†M ƒêI·ªÇM CHO FILE C≈® ---\n",
    "print(f\"ƒêang ƒë·ªçc v√† boost ƒëi·ªÉm cho {INPUT_FILE}...\")\n",
    "output_lines = []\n",
    "\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        # L·∫•y tr·ªçng s·ªë IC (N·∫øu kh√¥ng c√≥ trong train th√¨ m·∫∑c ƒë·ªãnh l√† hi·∫øm -> weight cao)\n",
    "        # Weight trung b√¨nh th∆∞·ªùng t·∫ßm 5-10. Ta scale xu·ªëng ch√∫t cho an to√†n.\n",
    "        weight = term_weights.get(term, 10.0) \n",
    "        \n",
    "        # LOGIC BOOST:\n",
    "        # Score m·ªõi = Score c≈© * (1 + Factor * IC)\n",
    "        # Factor 0.05 nghƒ©a l√† nh√£n hi·∫øm ƒë∆∞·ª£c th∆∞·ªüng th√™m t·∫ßm 20-50% ƒëi·ªÉm\n",
    "        boost_factor = 0.05 \n",
    "        new_score = score * (1 + boost_factor * weight)\n",
    "        \n",
    "        # Clip v·ªÅ 1.0\n",
    "        new_score = min(new_score, 1.0)\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i n·∫øu ƒëi·ªÉm s·ªë ƒë·ªß cao (L·ªçc r√°c k·ªπ h∆°n)\n",
    "        if new_score > 0.015:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{new_score:.3f}\")\n",
    "\n",
    "# --- B∆Ø·ªöC 3: L∆ØU FILE ---\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"XONG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ba1bc-30d9-4bf5-902c-4c74d2eeeba0",
   "metadata": {},
   "source": [
    "#### Score: 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26510a-bf8b-4ae2-8a99-898410d607c5",
   "metadata": {},
   "source": [
    "#### V√©t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6889f508-a97f-49e5-b938-a8718a991437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t√≠nh IC...\n",
      "ƒêang t·∫°o 3 file bi·∫øn th·ªÉ...\n",
      "-> T·∫°o submission_IC_v1_conservative.tsv (Boost=0.02, Thr=0.015)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> T·∫°o submission_IC_v2_aggressive.tsv (Boost=0.1, Thr=0.015)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> T·∫°o submission_IC_v3_loose_thr.tsv (Boost=0.05, Thr=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ t·∫°o xong 3 file! H√£y n·ªôp l·∫ßn l∆∞·ª£t ƒë·ªÉ t√¨m ra c·∫•u h√¨nh t·ªët nh·∫•t.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# D√πng file KNN g·ªëc (File 0.234 - tr∆∞·ªõc khi IC weight)\n",
    "# L∆∞u √Ω: Ph·∫£i l√† file \"submission_knn_tax_propagated.tsv\"\n",
    "INPUT_FILE = \"submission_knn_tax_propagated.tsv\"\n",
    "TRAIN_TERMS = \"/workspace/data/Train/train_terms.tsv\"\n",
    "\n",
    "# --- B∆Ø·ªöC 1: T√çNH L·∫†I IC (Gi·ªØ nguy√™n) ---\n",
    "print(\"ƒêang t√≠nh IC...\")\n",
    "df_terms = pd.read_csv(TRAIN_TERMS, sep=\"\\t\", usecols=['term'])\n",
    "term_counts = df_terms['term'].value_counts().to_dict()\n",
    "total_proteins = len(df_terms)\n",
    "term_weights = {}\n",
    "for term, count in term_counts.items():\n",
    "    term_weights[term] = -math.log2(count / total_proteins)\n",
    "\n",
    "# --- B∆Ø·ªöC 2: T·∫†O 3 BI·∫æN TH·ªÇ ---\n",
    "# C·∫•u h√¨nh c√°c chi·∫øn thu·∫≠t\n",
    "strategies = [\n",
    "    {\"name\": \"v1_conservative\", \"boost\": 0.02, \"thr\": 0.015}, # B∆°m nh·∫π\n",
    "    {\"name\": \"v2_aggressive\",   \"boost\": 0.10, \"thr\": 0.015}, # B∆°m m·∫°nh tay\n",
    "    {\"name\": \"v3_loose_thr\",    \"boost\": 0.05, \"thr\": 0.010}, # B∆°m v·ª´a, nh∆∞ng l·∫•y nhi·ªÅu nh√£n h∆°n\n",
    "]\n",
    "\n",
    "print(\"ƒêang t·∫°o 3 file bi·∫øn th·ªÉ...\")\n",
    "\n",
    "# ƒê·ªçc file g·ªëc v√†o b·ªô nh·ªõ cho nhanh\n",
    "with open(INPUT_FILE) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for strat in strategies:\n",
    "    out_name = f\"submission_IC_{strat['name']}.tsv\"\n",
    "    boost = strat['boost']\n",
    "    thr = strat['thr']\n",
    "    \n",
    "    print(f\"-> T·∫°o {out_name} (Boost={boost}, Thr={thr})...\")\n",
    "    out_lines = []\n",
    "    \n",
    "    for line in tqdm(lines, leave=False):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        weight = term_weights.get(term, 10.0)\n",
    "        \n",
    "        # C√¥ng th·ª©c boost\n",
    "        new_score = score * (1 + boost * weight)\n",
    "        new_score = min(new_score, 1.0)\n",
    "        \n",
    "        if new_score > thr:\n",
    "            out_lines.append(f\"{pid}\\t{term}\\t{new_score:.3f}\")\n",
    "            \n",
    "    with open(out_name, \"w\") as f:\n",
    "        f.write(\"\\n\".join(out_lines))\n",
    "\n",
    "print(\"ƒê√£ t·∫°o xong 3 file! H√£y n·ªôp l·∫ßn l∆∞·ª£t ƒë·ªÉ t√¨m ra c·∫•u h√¨nh t·ªët nh·∫•t.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
