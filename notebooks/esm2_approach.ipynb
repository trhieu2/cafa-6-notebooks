{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb88d2c5-f7b1-49cd-9e45-df552f867a80",
   "metadata": {},
   "source": [
    "### Drawback of traditional ML approach: Kh√¥ng gi·ªØ ƒë∆∞·ª£c order meaning c·ªßa chu·ªói axit amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac6754-31c2-4d28-9654-8867e3b45e60",
   "metadata": {},
   "source": [
    "#### Sol: s·ª≠ d·ª•ng m√¥ h√¨nh esm2 650M tham s·ªë ƒë·ªÉ embedding c√°c chu·ªói acid amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa35b0-af98-41c1-89bc-6e426b6dd599",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69266e5f-6de0-4045-89bc-4f204724623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c681d99-6943-4c20-9dea-c2983627f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87397ece-1f18-4462-8662-9444fcc2fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/esm2_t33_650M_UR50D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-32): 33 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval() #read-only to save VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fc7c8-dabb-4934-a152-6e57c5befb31",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f609ee-33d4-4bbf-b76a-669d20fe9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(fasta_path, save_name, batch_size=8, limit=None):\n",
    "    \"\"\"\n",
    "    Read fasta file -> Run through ESM2 -> output: .npy\n",
    "    save_name: output file\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    sequences = []\n",
    "\n",
    "    print(f\"Reading file: {fasta_path}\")\n",
    "    for i, record in enumerate(SeqIO.parse(fasta_path, \"fasta\")):\n",
    "        if limit and i >= limit: break\n",
    "\n",
    "        #clean id\n",
    "        pid = str(record.id)\n",
    "        if \"|\" in pid:\n",
    "            pid = pid.split(\"|\")[1]\n",
    "\n",
    "        ids.append(pid)\n",
    "        #esm2 limit 1024 token\n",
    "        sequences.append(str(record.seq)[:1022])\n",
    "\n",
    "    print(f\"{len(sequences)} Proteins\")\n",
    "\n",
    "    #batching\n",
    "    embeddings = []\n",
    "    print(\"Creating embeddings...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        batch_seqs = sequences[i : i + batch_size]\n",
    "\n",
    "        #tokenize\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #mean pooling\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "        sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "    final_embeddings = np.vstack(embeddings)\n",
    "\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}.npy\", final_embeddings)\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}_ids.npy\", ids)\n",
    "\n",
    "    return ids, final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9cc34-a585-4965-924b-0e4bfa46cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "\n",
    "train_ids, X_train = extract_embeddings(\n",
    "    train_fasta, \n",
    "    save_name=\"train_650M\", \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516fd8c-8525-4a68-aaef-2b7898aa4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#prepare labels\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set)]\n",
    "\n",
    "top_n = 1500\n",
    "top_terms = train_terms_filtered[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "\n",
    "Y_matrix = train_terms_filtered[train_terms_filtered[\"term\"].isin(top_terms)] \\\n",
    "            .pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b21e0-2840-473a-9924-d77f53b286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c7e45-86be-492c-be7a-d371b03853f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df688e-8fca-4eae-af99-e1c77f6b01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_val = clf.predict(X_val)\n",
    "score = f1_score(Y_val, Y_pred_val, average='micro')\n",
    "print(f\"Local F1-Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073505a7-5e08-4218-9324-aa702bb1858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell ch·∫©n ƒëo√°n l·ªói ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 1. Ki·ªÉm tra xem m√¥ h√¨nh c√≥ d·ª± ƒëo√°n ra c√°i g√¨ kh√¥ng?\n",
    "print(f\"T·ªïng s·ªë m·∫´u trong t·∫≠p Val: {Y_val.shape[0]}\")\n",
    "print(f\"T·ªïng s·ªë nh√£n c·∫ßn d·ª± ƒëo√°n: {Y_val.shape[0] * Y_val.shape[1]}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (Th·ª±c t·∫ø): {Y_val.sum()}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (M√¥ h√¨nh d·ª± ƒëo√°n): {Y_pred_val.sum()}\")\n",
    "\n",
    "# 2. N·∫øu s·ªë d·ª± ƒëo√°n qu√° th·∫•p (g·∫ßn b·∫±ng 0), ta c·∫ßn h·∫° ng∆∞·ª°ng (Threshold)\n",
    "print(\"\\n--- Th·ª≠ ch·ªânh ng∆∞·ª°ng th·ªß c√¥ng ---\")\n",
    "# L·∫•y ƒëi·ªÉm s·ªë th√¥ thay v√¨ nh√£n c·ª©ng 0/1\n",
    "decision_scores = clf.decision_function(X_val) \n",
    "\n",
    "# Th·ª≠ c√°c ng∆∞·ª°ng kh√°c nhau\n",
    "for thr in [0, -0.5, -1.0]: # Ridge score c√≥ th·ªÉ √¢m\n",
    "    y_pred_new = (decision_scores > thr).astype(int)\n",
    "    new_f1 = f1_score(Y_val, y_pred_new, average='micro')\n",
    "    print(f\"Ng∆∞·ª°ng {thr}: F1-Score = {new_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6d097d-5ca1-4459-a286-95445364e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [1:46:48<00:00, 47.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "FASTA_PATH = \"/workspace/data/Test/testsuperset.fasta\" \n",
    "SAVE_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "CHUNK_SIZE = 5000  \n",
    "BATCH_SIZE = 8    \n",
    "\n",
    "# --- SETUP ---\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "def process_and_save(seqs, ids, part_idx):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(seqs), BATCH_SIZE):\n",
    "        batch_seqs = seqs[i : i + BATCH_SIZE]\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "            sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "    \n",
    "    final_emb = np.vstack(embeddings)\n",
    "    np.save(f\"{SAVE_DIR}/test_part_{part_idx}.npy\", final_emb)\n",
    "    np.save(f\"{SAVE_DIR}/test_ids_{part_idx}.npy\", ids)\n",
    "\n",
    "sequences = []\n",
    "ids = []\n",
    "part_counter = 0\n",
    "\n",
    "pbar = tqdm(total=224309) \n",
    "\n",
    "for record in SeqIO.parse(FASTA_PATH, \"fasta\"):\n",
    "    save_path_check = f\"{SAVE_DIR}/test_part_{part_counter}.npy\"\n",
    "    \n",
    "    if os.path.exists(save_path_check):\n",
    "        sequences.append(1) \n",
    "        if len(sequences) >= CHUNK_SIZE:\n",
    "            sequences = [] \n",
    "            ids = []\n",
    "            part_counter += 1\n",
    "            pbar.update(CHUNK_SIZE)\n",
    "        continue\n",
    "\n",
    "    pid = str(record.id).split(\"|\")[1] if \"|\" in str(record.id) else str(record.id)\n",
    "    ids.append(pid)\n",
    "    sequences.append(str(record.seq)[:1022])\n",
    "    pbar.update(1)\n",
    "    \n",
    "    if len(sequences) >= CHUNK_SIZE: \n",
    "        process_and_save(sequences, ids, part_counter)\n",
    "        part_counter += 1\n",
    "        \n",
    "        sequences = []\n",
    "        ids = []\n",
    "        gc.collect()\n",
    "\n",
    "if len(sequences) > 0:\n",
    "    process_and_save(sequences, ids, part_counter)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ade8a3-f0ab-48ed-ba9a-2951ef9a5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\" \n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_CHUNKS_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE = \"submission_level4_FINAL_fixed.tsv\"\n",
    "\n",
    "# Re-training\n",
    "print(\"Loading data and training model...\")\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "top_n = 1500\n",
    "top_terms = train_terms[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set) & train_terms[\"term\"].isin(top_terms)]\n",
    "\n",
    "Y_matrix = train_terms_filtered.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)\n",
    "terms_columns = Y_train.columns \n",
    "\n",
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "del X_train, Y_train, train_terms, train_terms_filtered, Y_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"Outputting (Top K)...\")\n",
    "\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "TOP_K = 50   \n",
    "THRESHOLD = 0.01 #increased threshold\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    X_chunk = np.load(f_path)\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_chunk = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    #predicting\n",
    "    decision_scores = clf.decision_function(X_chunk)\n",
    "    probs = 1 / (1 + np.exp(-decision_scores))\n",
    "    \n",
    "    for i, pid in enumerate(ids_chunk):\n",
    "        prob_row = probs[i]\n",
    "        \n",
    "        # 1. thresholding\n",
    "        mask = prob_row > THRESHOLD\n",
    "        if not np.any(mask):\n",
    "            indices = np.argsort(prob_row)[-5:]\n",
    "        else:\n",
    "            candidates = np.where(mask)[0]\n",
    "            \n",
    "            #2: only select the top 50\n",
    "            if len(candidates) > TOP_K:\n",
    "                # get candidate scores\n",
    "                cand_probs = prob_row[candidates]\n",
    "                # sort for top k\n",
    "                top_k_local_idx = np.argsort(cand_probs)[-TOP_K:]\n",
    "                indices = candidates[top_k_local_idx]\n",
    "            else:\n",
    "                indices = candidates\n",
    "            \n",
    "        for idx in indices:\n",
    "            term = terms_columns[idx]\n",
    "            score = prob_row[idx]\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "            \n",
    "    del X_chunk, ids_chunk, decision_scores, probs\n",
    "    gc.collect()\n",
    "\n",
    "# Output file\n",
    "print(f\"üíæ Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a3be4-23ae-472b-a210-f280f2013ff1",
   "metadata": {},
   "source": [
    "#### Score: 0.192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af3504-0f41-4ad2-9552-fa926369c9a6",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998d396-61f8-462b-9e2b-5163034c552a",
   "metadata": {},
   "source": [
    "### GO Hierarchy: Ridge classifier ƒëang h·ªçc c√°c nh√£n 1 c√°ch ƒë·ªôc l·∫≠p, nh√£n con c√≥ th·ªÉ c√≥ score cao, nh∆∞ng nh·ªØng nh√£n cha chung chung th√¨ score l·∫°i th·∫•p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0556a-def4-4de0-bbe2-5526eac4b75d",
   "metadata": {},
   "source": [
    "### Sol: Ensemble: Mix v·ªõi naive approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9861fdff-8d1b-4719-983a-02e2947cbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ESM2 output file....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:07, 1548396.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Naive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10093905it [00:05, 1690414.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17589919/17589919 [00:20<00:00, 844235.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /workspace/notebooks/submission_ensemble_boosted.tsv\n",
      "Finisehd.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEVEL4_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "NAIVE_FILE = \"/workspace/notebooks/submission_naive.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_ensemble_boosted.tsv\"\n",
    "\n",
    "print(\"Reading ESM2 output file....\")\n",
    "preds_l4 = {}\n",
    "\n",
    "try:\n",
    "    with open(LEVEL4_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            # Key : (ProteinID, GO_Term)\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_l4[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {LEVEL4_FILE}.\")\n",
    "    raise\n",
    "\n",
    "print(\"Reading file Naive...\")\n",
    "preds_naive = {}\n",
    "try:\n",
    "    with open(NAIVE_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_naive[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {NAIVE_FILE}\")\n",
    "    raise\n",
    "\n",
    "print(\"Ensembling...\")\n",
    "\n",
    "#Select all id-term pairs in 2 files\n",
    "all_keys = set(preds_l4.keys()) | set(preds_naive.keys())\n",
    "output_lines = []\n",
    "\n",
    "W_L4 = 0.6\n",
    "W_NAIVE = 0.4\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    # L·∫•y ƒëi·ªÉm s·ªë, n·∫øu file n√†o kh√¥ng c√≥ th√¨ coi l√† 0\n",
    "    score_l4 = preds_l4.get(key, 0.0)\n",
    "    score_naive = preds_naive.get(key, 0.0)\n",
    "    \n",
    "    # C√¥ng th·ª©c c·ªông g·ªôp\n",
    "    final_score = (score_l4 * W_L4) + (score_naive * W_NAIVE)\n",
    "    \n",
    "    # Ch·ªâ ghi nh·ªØng d√≤ng c√≥ ƒëi·ªÉm s·ªë > 0.001 ƒë·ªÉ file ƒë·ª° n·∫∑ng\n",
    "    if final_score > 0.001:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "# Ghi ra file\n",
    "print(f\"Saving: {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"Finisehd.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fccd1-0fb3-4f0a-bb2d-8f2b5f791e3e",
   "metadata": {},
   "source": [
    "### Ensemble ESM2 + BLAST/Diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a522344-5b1a-4176-89c3-3c581bc841e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnign BLAST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diamond v2.1.8.162 (C) Max Planck Society for the Advancement of Science, Benjamin Buchfink, University of Tuebingen\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "Please cite: http://dx.doi.org/10.1038/s41592-021-01101-x Nature Methods (2021)\n",
      "\n",
      "#CPU threads: 32\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Temporary directory: /workspace/notebooks\n",
      "Percentage range of top alignment score to report hits: 1\n",
      "Opening the database...  [0.06s]\n",
      "Database: /workspace/data/Traintrain_data.dmnd (type: Diamond database, sequences: 82404, letters: 43327058)\n",
      "Block size = 2000000000\n",
      "Opening the input file...  [0.035s]\n",
      "Opening the output file...  [0s]\n",
      "Loading query sequences...  [0.215s]\n",
      "Masking queries...  [0.141s]\n",
      "Algorithm: Double-indexed\n",
      "Building query histograms...  [0.513s]\n",
      "Seeking in database...  [0s]\n",
      "Loading reference sequences...  [0.051s]\n",
      "Masking reference...  [0.061s]\n",
      "Initializing temporary storage...  [0s]\n",
      "Building reference histograms...  [0.22s]\n",
      "Allocating buffers...  [0.002s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.038s]\n",
      "Computing hash join...  [0.037s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.431s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.36s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.338s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.021s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.039s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.313s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.365s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.338s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.35s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.041s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.374s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.045s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.335s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.336s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.022s]\n",
      "Building query seed array...  [0.049s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.013s]\n",
      "Searching alignments...  [0.318s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.309s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.344s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.329s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.317s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.036s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.051s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.325s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.318s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.036s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.349s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.341s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.328s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.34s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.33s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.324s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.343s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.332s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.035s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.315s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.347s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.335s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.319s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.352s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.04s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.334s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.02s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.323s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.32s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.342s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.\n",
      "Building reference seed array...  [0.018s]\n",
      "Building query seed array...  [0.043s]\n",
      "Computing hash join...  [0.034s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.331s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.\n",
      "Building reference seed array...  [0.019s]\n",
      "Building query seed array...  [0.042s]\n",
      "Computing hash join...  [0.032s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.322s]\n",
      "Deallocating memory...  [0s]\n",
      "Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.035s]\n",
      "Computing hash join...  [0.033s]\n",
      "Masking low complexity seeds...  [0.012s]\n",
      "Searching alignments...  [0.313s]\n",
      "Deallocating memory...  [0s]\n",
      "Deallocating buffers...  [0.019s]\n",
      "Clearing query masking...  [0.006s]\n",
      "Computing alignments... Loading trace points...  [0.772s]\n",
      "Sorting trace points...  [0.214s]\n",
      "Computing alignments...  [9.531s]\n",
      "Deallocating buffers...  [0.061s]\n",
      "Loading trace points...  [0s]\n",
      " [10.659s]\n",
      "Deallocating reference...  [0.002s]\n",
      "Loading reference sequences...  [0s]\n",
      "Deallocating buffers...  [0s]\n",
      "Deallocating queries...  [0.005s]\n",
      "Loading query sequences...  [0s]\n",
      "Closing the input file...  [0s]\n",
      "Closing the output file...  [0s]\n",
      "Closing the database...  [0.003s]\n",
      "Cleaning up...  [0s]\n",
      "Total time = 40.168s\n",
      "Reported 262530 pairwise alignments, 262530 HSPs.\n",
      "204232 queries aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished blasting\n",
      "Handling output of BLAST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262530/262530 [00:03<00:00, 72338.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Level 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:07, 1560616.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11968960/11968960 [00:12<00:00, 950665.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /workspace/notebooks/submission_hybrid_blast_esm2.tsv...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEVEL4_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "\n",
    "DIAMOND_BIN = \"/usr/bin/diamond\"\n",
    "TRAIN_FASTA = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "TEST_FASTA = \"/workspace/data/Test/testsuperset.fasta\"\n",
    "DB_PATH = \"/workspace/data/Traintrain_data.dmnd\"\n",
    "BLAST_RESULT = \"/workspace/notebooks/diamond_results.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_hybrid_blast_esm2.tsv\"\n",
    "\n",
    "print(\"Runnign BLAST...\")\n",
    "\n",
    "#create db\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"ƒêang t·∫°o database...\")\n",
    "    os.system(f\"{DIAMOND_BIN} makedb --in {TRAIN_FASTA} -d {DB_PATH} --quiet\")\n",
    "\n",
    "#alignment\n",
    "cmd = f\"{DIAMOND_BIN} blastp -d {DB_PATH} -q {TEST_FASTA} -o {BLAST_RESULT} --sensitive --top 1 -f 6 qseqid sseqid pident\"\n",
    "os.system(cmd)\n",
    "print(\"Finished blasting\")\n",
    "\n",
    "print(\"Handling output of BLAST...\")\n",
    "blast_preds = {}\n",
    "\n",
    "# Load train terms ƒë·ªÉ map nh√£n\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "train_terms_grouped = train_terms.groupby(\"EntryID\")[\"term\"].apply(list).to_dict()\n",
    "\n",
    "# ƒê·ªçc k·∫øt qu·∫£ BLAST\n",
    "df_blast = pd.read_csv(BLAST_RESULT, sep=\"\\t\", names=[\"test_id\", \"train_id\", \"pident\"])\n",
    "\n",
    "for _, row in tqdm(df_blast.iterrows(), total=len(df_blast)):\n",
    "    # Clean ID (S·ª≠a l·ªói ID b·ªã d√≠nh sp|...)\n",
    "    test_id = str(row['test_id']).split(\"|\")[1] if \"|\" in str(row['test_id']) else str(row['test_id'])\n",
    "    train_id = str(row['train_id']).split(\"|\")[1] if \"|\" in str(row['train_id']) else str(row['train_id'])\n",
    "    \n",
    "    score = row['pident'] / 100.0\n",
    "    \n",
    "    # Ch·ªâ l·∫•y nh·ªØng th·∫±ng gi·ªëng nhau > 30% (Ng∆∞·ª°ng an to√†n)\n",
    "    if score < 0.3: continue\n",
    "    \n",
    "    if train_id in train_terms_grouped:\n",
    "        for term in train_terms_grouped[train_id]:\n",
    "            key = (test_id, term)\n",
    "            # BLAST r·∫•t uy t√≠n, n√™n gi·ªØ nguy√™n score cao\n",
    "            blast_preds[key] = score\n",
    "\n",
    "print(\"Reading file Level 4...\")\n",
    "esm_preds = {}\n",
    "try:\n",
    "    with open(LEVEL4_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            key = (parts[0], parts[1])\n",
    "            esm_preds[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {LEVEL4_FILE}.\")\n",
    "    raise\n",
    "\n",
    "print(\"Mixing...\")\n",
    "\n",
    "all_keys = set(blast_preds.keys()) | set(esm_preds.keys())\n",
    "output_lines = []\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    s_blast = blast_preds.get(key, 0.0)\n",
    "    s_esm = esm_preds.get(key, 0.0)\n",
    "    \n",
    "    # CHI·∫æN THU·∫¨T QUAN TR·ªåNG:\n",
    "    # N·∫øu BLAST t√¨m th·∫•y -> Tin BLAST (v√¨ n√≥ so kh·ªõp ch√≠nh x√°c)\n",
    "    # N·∫øu BLAST kh√¥ng th·∫•y -> Tin ESM (v√¨ n√≥ suy lu·∫≠n t·ªët)\n",
    "    # => L·∫•y MAX\n",
    "    final_score = max(s_blast, s_esm)\n",
    "    \n",
    "    output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "#GHI FILE\n",
    "print(f\"Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6ff76-d373-4e12-8fa9-9d35ffe443cd",
   "metadata": {},
   "source": [
    "### Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558f6ac7-0685-4bab-8850-ea30b7db9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting obonet\n",
      "  Downloading obonet-1.1.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (3.3)\n",
      "Downloading obonet-1.1.1-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: obonet\n",
      "Successfully installed obonet-1.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install obonet networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294bd6ab-f94a-414d-a360-34449af1c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_level4_propagated.tsv\"\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a5e59-213b-4ba7-9958-eeb0f7cfca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªçc c√¢y ph·∫£ h·ªá...\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒê·ªçc c√¢y ph·∫£ h·ªá...\")\n",
    "graph = obonet.read_obo(OBO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0300ecaf-ed1b-421c-a5ac-99f6023abff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X√¢y map quan h·ªá cha-con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 132343.17it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"X√¢y map quan h·ªá cha-con...\")\n",
    "ancestors_map = {}\n",
    "for node in tqdm(graph.nodes()):\n",
    "    # networkx tr·∫£ v·ªÅ danh s√°ch t·ªï ti√™n\n",
    "    try:\n",
    "        ancestors = networkx.descendants(graph, node) # Trong obonet, chi·ªÅu m≈©i t√™n ng∆∞·ª£c (Con -> Cha)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feda38f8-9b19-43d5-8aa2-1b16cf5de9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc /workspace/notebooks/submission_level4_FINAL_fixed.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:04, 2284548.44it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang ƒë·ªçc {INPUT_FILE}...\")\n",
    "# ƒê·ªçc v√†o Dict: {ProteinID: {Term: Score}}\n",
    "preds = {}\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        if pid not in preds: preds[pid] = {}\n",
    "        preds[pid][term] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99afb86c-c69c-4f47-b9ca-5d5e81f17f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lan truy·ªÅn ng∆∞·ª£c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:22<00:00, 9829.63it/s] \n"
     ]
    }
   ],
   "source": [
    "print(\"Lan truy·ªÅn ng∆∞·ª£c...\")\n",
    "final_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # term_scores l√† dict {Term: Score g·ªëc}\n",
    "    # new_scores s·∫Ω ch·ª©a c·∫£ ƒëi·ªÉm c·ªßa cha √¥ng\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    for term, score in term_scores.items():\n",
    "        # L·∫•y danh s√°ch cha √¥ng c·ªßa term n√†y\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # Quy t·∫Øc: ƒêi·ªÉm c·ªßa cha = MAX(ƒêi·ªÉm cha c≈©, ƒêi·ªÉm c·ªßa con)\n",
    "                current_parent_score = new_scores.get(parent, 0.0)\n",
    "                new_scores[parent] = max(current_parent_score, score)\n",
    "    \n",
    "    # Ghi ra k·∫øt qu·∫£ (L·∫°i ph·∫£i l·ªçc Top K v√¨ gi·ªù n√≥ ph√¨nh to ra)\n",
    "    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # L·∫•y Top 70 (tƒÉng l√™n ch√∫t v√¨ gi·ªù c√≥ c·∫£ cha √¥ng)\n",
    "    for term, score in sorted_terms[:70]:\n",
    "        final_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e773adc4-a1ea-4912-ba82-05b61497296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /workspace/notebooks/submission_level4_propagated.tsv...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(final_lines))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecdb47-8b91-4fa1-ae90-77d46dfe8faa",
   "metadata": {},
   "source": [
    "### S·ª≠ d·ª•ng KNN clf tr√™n embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91df6974-1b5f-4d07-9884-9aff772279d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\"\n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_CHUNKS_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE = \"submission_knn_esm2.tsv\"\n",
    "TOP_K = 5  # L·∫•y 5 ng∆∞·ªùi h√†ng x√≥m gi·ªëng nh·∫•t\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f43d68d-8828-4626-b65b-f6e4c935c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Train Embeddings...\n",
      "Loading labels...\n",
      "Train xong tren GPU!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Train Embeddings...\")\n",
    "# Load vector\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "X_train = torch.from_numpy(X_train).to(device)\n",
    "\n",
    "# Chu·∫©n h√≥a vector v·ªÅ ƒë∆°n v·ªã (ƒë·ªÉ t√≠nh Cosine Similarity nhanh b·∫±ng ph√©p nh√¢n ma tr·∫≠n)\n",
    "# C√¥ng th·ª©c: v = v / |v|\n",
    "norm = X_train.norm(p=2, dim=1, keepdim=True)\n",
    "X_train = X_train.div(norm)\n",
    "\n",
    "# Load ID v√† Map Nh√£n\n",
    "print(\"Loading labels...\")\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "# Gom nh√≥m: TrainID -> Set(Terms)\n",
    "train_labels_map = train_terms.groupby(\"EntryID\")[\"term\"].apply(set).to_dict()\n",
    "\n",
    "# Map Index -> ID (ƒë·ªÉ truy xu·∫•t nhanh t·ª´ k·∫øt qu·∫£ KNN)\n",
    "idx_to_trainid = {i: pid for i, pid in enumerate(train_ids)}\n",
    "\n",
    "print(\"Train xong tren GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e6e9ad-aca9-4109-b720-f37a93f1ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu ch·∫°y KNN (T√¨m h√†ng x√≥m)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:05<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu ch·∫°y KNN (T√¨m h√†ng x√≥m)...\")\n",
    "\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    # 1. Load 1 c·ª•c Test l√™n GPU\n",
    "    X_test_np = np.load(f_path)\n",
    "    X_test = torch.from_numpy(X_test_np).to(device)\n",
    "    \n",
    "    # Chu·∫©n h√≥a Test\n",
    "    norm_test = X_test.norm(p=2, dim=1, keepdim=True)\n",
    "    X_test = X_test.div(norm_test)\n",
    "    \n",
    "    # Load ID Test t∆∞∆°ng ·ª©ng\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_test = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    # 2. T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng (Matrix Multiplication)\n",
    "    # [Batch, 1280] x [1280, All_Train] = [Batch, All_Train]\n",
    "    # ƒê√¢y l√† b∆∞·ªõc n·∫∑ng nh·∫•t, nh∆∞ng GPU x·ª≠ l√Ω t·ªët\n",
    "    sim_matrix = torch.mm(X_test, X_train.t())\n",
    "    \n",
    "    # 3. L·∫•y Top K h√†ng x√≥m\n",
    "    # values: ƒë·ªô t∆∞∆°ng ƒë·ªìng (score), indices: v·ªã tr√≠ c·ªßa h√†ng x√≥m\n",
    "    topk_values, topk_indices = torch.topk(sim_matrix, k=TOP_K, dim=1)\n",
    "    \n",
    "    # Chuy·ªÉn v·ªÅ CPU ƒë·ªÉ x·ª≠ l√Ω logic g√°n nh√£n (Python x·ª≠ l√Ω dict nhanh h∆°n)\n",
    "    topk_indices = topk_indices.cpu().numpy()\n",
    "    topk_values = topk_values.cpu().numpy()\n",
    "    \n",
    "    # 4. T·ªïng h·ª£p nh√£n t·ª´ h√†ng x√≥m (Weighted Voting)\n",
    "    for i, test_pid in enumerate(ids_test):\n",
    "        # Dict l∆∞u ƒëi·ªÉm s·ªë cho t·ª´ng nh√£n: {Term: Score}\n",
    "        term_scores = {}\n",
    "        \n",
    "        for k in range(TOP_K):\n",
    "            neighbor_idx = topk_indices[i, k]\n",
    "            score = topk_values[i, k] # ƒê·ªô gi·ªëng nhau (v√≠ d·ª• 0.95)\n",
    "            \n",
    "            neighbor_id = idx_to_trainid[neighbor_idx]\n",
    "            \n",
    "            # N·∫øu h√†ng x√≥m n√†y c√≥ nh√£n (c√≥ trong file train_terms)\n",
    "            if neighbor_id in train_labels_map:\n",
    "                neighbor_terms = train_labels_map[neighbor_id]\n",
    "                for term in neighbor_terms:\n",
    "                    # C·ªông d·ªìn ƒëi·ªÉm (Weighted Sum)\n",
    "                    if term not in term_scores:\n",
    "                        term_scores[term] = 0.0\n",
    "                    term_scores[term] += score\n",
    "        \n",
    "        # Chu·∫©n h√≥a ƒëi·ªÉm s·ªë (Chia cho t·ªïng tr·ªçng s·ªë ho·∫∑c K)\n",
    "        # ·ªû ƒë√¢y ta chia cho K ƒë·ªÉ score n·∫±m trong kho·∫£ng 0-1\n",
    "        # Ho·∫∑c ƒë∆°n gi·∫£n l√† gi·ªØ nguy√™n v√¨ CAFA ch·∫•m rank\n",
    "        \n",
    "        # L·∫•y Top 50 nh√£n ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ ghi file\n",
    "        sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "        \n",
    "        for term, total_score in sorted_terms:\n",
    "            # Normalize heuristic: Score trung b√¨nh\n",
    "            final_score = total_score / TOP_K \n",
    "            # Ch·ªâ ghi n·∫øu score ƒë·ªß l·ªõn\n",
    "            if final_score > 0.01: \n",
    "                output_lines.append(f\"{test_pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "    # D·ªçn d·∫πp b·ªô nh·ªõ GPU\n",
    "    del X_test, sim_matrix, topk_values, topk_indices\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d74bf97-e061-4483-8957-2a15ce2d462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u submission_knn_esm2.tsv...\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8eb4d-2a29-49ad-9fa1-a055d73e2fed",
   "metadata": {},
   "source": [
    "#### + Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d76c71-b46c-45a1-bcf2-fdb531185c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"/workspace/notebooks/submission_knn_esm2.tsv\" \n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_knn_propagated.tsv\"\n",
    "OBO_PATH = \"/workspace/data/Train/go-basic.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1f094f-447a-4511-b7f7-f891878a82dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªçc file go-basic.obo...\n",
      "ƒêang x√¢y d·ª±ng quan h·ªá t·ªï ti√™n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40122/40122 [00:00<00:00, 158229.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë·ªçc /workspace/notebooks/submission_knn_esm2.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4549325it [00:01, 2344697.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:12<00:00, 18368.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u /workspace/notebooks/submission_knn_propagated.tsv...\n",
      "XONG!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load c√¢y Gene Ontology\n",
    "print(\"ƒê·ªçc file go-basic.obo...\")\n",
    "graph = obonet.read_obo(OBO_PATH)\n",
    "\n",
    "# 2. X√¢y d·ª±ng b·∫£n ƒë·ªì Cha-Con\n",
    "print(\"ƒêang x√¢y d·ª±ng quan h·ªá t·ªï ti√™n...\")\n",
    "ancestors_map = {}\n",
    "# Ch·ªâ quan t√¢m ƒë·∫øn c√°c node c√≥ quan h·ªá 'is_a' v√† 'part_of'\n",
    "for node in tqdm(graph.nodes()):\n",
    "    try:\n",
    "        # L·∫•y t·∫•t c·∫£ t·ªï ti√™n c·ªßa node hi·ªán t·∫°i\n",
    "        ancestors = networkx.descendants(graph, node)\n",
    "        ancestors_map[node] = ancestors\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 3. ƒê·ªçc file KNN\n",
    "print(f\"ƒêang ƒë·ªçc {INPUT_FILE}...\")\n",
    "preds = {} # {ProteinID: {Term: Score}}\n",
    "\n",
    "with open(INPUT_FILE) as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3: continue\n",
    "        pid, term, score = parts[0], parts[1], float(parts[2])\n",
    "        \n",
    "        if pid not in preds: preds[pid] = {}\n",
    "        preds[pid][term] = score\n",
    "\n",
    "# 4. Lan truy·ªÅn ƒëi·ªÉm s·ªë (Propagation)\n",
    "print(\"ƒêang lan truy·ªÅn ƒëi·ªÉm s·ªë (Fill Parents)...\")\n",
    "output_lines = []\n",
    "\n",
    "for pid, term_scores in tqdm(preds.items()):\n",
    "    # Copy dict ƒëi·ªÉm c≈©\n",
    "    new_scores = term_scores.copy()\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng term ƒëang c√≥\n",
    "    for term, score in term_scores.items():\n",
    "        # N·∫øu term n√†y c√≥ t·ªï ti√™n\n",
    "        if term in ancestors_map:\n",
    "            parents = ancestors_map[term]\n",
    "            for parent in parents:\n",
    "                # Quy t·∫Øc c·ªët l√µi: ƒêi·ªÉm c·ªßa Cha lu√¥n >= ƒêi·ªÉm c·ªßa Con\n",
    "                current_p_score = new_scores.get(parent, 0.0)\n",
    "                new_scores[parent] = max(current_p_score, score)\n",
    "    \n",
    "    # L·ªçc v√† Ghi file\n",
    "    # Sau khi lan truy·ªÅn, s·ªë l∆∞·ª£ng nh√£n s·∫Ω ph√¨nh to ra (v√¨ th√™m cha √¥ng)\n",
    "    # Ta ch·ªâ l·∫•y Top 70 nh√£n ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ file kh√¥ng qu√° n·∫∑ng\n",
    "    sorted_terms = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n",
    "    \n",
    "    for term, score in sorted_terms:\n",
    "        # L·ªçc b·ªõt r√°c: Ch·ªâ l·∫•y > 0.01\n",
    "        if score > 0.01:\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "# 5. L∆∞u file\n",
    "print(f\"ƒêang l∆∞u {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"XONG!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebab284-295d-45c2-bae2-fa2951ba0c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
